<!DOCTYPE html>
<html>
  <head>
    <title>Introduction to Machine Learning in R</title>
    <meta charset="utf-8">
    <meta name="author" content="D-Lab - Evan Muzzall and Chris Kennedy" />
    <link rel="stylesheet" href="scrollable.css" type="text/css" />
    <link rel="stylesheet" href="mtheme_max.css" type="text/css" />
    <link rel="stylesheet" href="fonts_mtheme_max.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">




class: title-slide   
&lt;a href="https://github.com/dlab-berkeley/Machine-Learning-in-R"&gt;&lt;img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png" alt="Fork me on GitHub"&gt;&lt;/a&gt;


&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;
# .font130[Introduction to Machine Learning in R]

### Evan Muzzall and Chris Kennedy  
### February 2, 2019

---

class: center, middle, inverse

# "It’s tough to make predictions, especially about the future." -Yogi Berra

---

# D-Lab 
[Visit the UC Berkeley D-Lab](http://dlab.berkeley.edu/) to learn more about our services and resources, [including the Machine Learning Working Group](http://dlab.berkeley.edu/working-groups/machine-learning-working-group-0).  

---

# Resources
_An Introduction to Statistical Learning - with Applications in R (2013)_ by Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani. Springer: New York. [Amazon](https://www.amazon.com/Introduction-Statistical-Learning-Applications-Statistics/dp/1461471370) or [free PDF](http://www-bcf.usc.edu/~gareth/ISL/). We encourage you to support the authors by purchasing their textbook!  

[Also check out the many resources](https://github.com/dlab-berkeley/MachineLearningWG) outlined in D-Lab's Machine Learning Working Group repository. 

---

# Software requirements

[Click here to install R 3.4 or greater](https://cloud.r-project.org/)   

[Click here to install RStudio](https://www.rstudio.com/products/rstudio/download/)  

---

# Download materials

[Click here to download the Introduction to Machine Learning in R workshop materials](https://github.com/dlab-berkeley/Machine-Learning-in-R)
  * Click green “Clone or Download” button
  * Click “Download Zip”
  * Extract this zip file to your Dropbox / Box / etc. and double-click the Rproj file.
  
Or (advanced):
  * Copy the github clone URL (https or ssh version)
  * In RStudio select File -&gt; New Project -&gt; Version Control -&gt; Git and paste the repository URL

---

# Install required R packages 

This workshop will utilize a variety of packages to install and organize the code, fit the machine learning algorithms, visualize their outputs, and evaluate their performances. 

* Algorithms: "gbm", "randomForest", "ranger", "rpart", "xgboost"  
* Visualization: "ggplot2", "rpart.plot"  
* Machine learning frameworks: "caret", "SuperLearner"  
* R utility packages: "devtools", "dplyr"  
* Miscellaneous: "mlbench", "pROC"

### Manually run the lone code chunk inside 1-overview.Rmd to install the packages


---

# Brief history of machine learning

Machine learning evolved from scientific pursuits in statistics, computer science, information theory, artificial intelligence, and pattern recognition.  

How to define machine learning?  
1) **In general:** algorithms, computers, and other machines that can "learn" without direct input from a human programmer.  
2) **Practically:** sets of tools for investigating/modeling/understanding data.  
3)  **Specifically:** (see below)

---

# Brief history of machine learning

A proto-example:  
- [Pascal's calculator](http://history-computer.com/MechanicalCalculators/Pioneers/Pascal.html)  

Rapid advances:   
- [McCulloch Pitts neuron model](http://wwwold.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node12.html)  
- [Turing test](http://www.jstor.org/stable/pdf/2251299.pdf)  
- [Rosenblatt's perceptron](http://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf)  
- [Samuels and the game of checkers](http://ucelinks.cdlib.org:8888/sfx_local?sid=google&amp;auinit=AL&amp;aulast=Samuel&amp;atitle=Some+studies+in+machine+learning+using+the+game+of+checkers&amp;id=doi:10.1147/rd.33.0210&amp;title=IBM+Journal+of+Research+and+Development&amp;volume=3&amp;issue=3&amp;date=1959&amp;spage=210&amp;issn=0018-8646)  

Modern topics:  
- [Turing Test: 50 years later](http://www.cs.bilkent.edu.tr/~akman/jour-papers/mam/mam2000.pdf)  
- [computer vision](http://www.sciencedirect.com/science/article/pii/S1071581916301264)  
- [data cleaning](http://www.betterevaluation.org/sites/default/files/data_cleaning.pdf)  
- [robotics](https://arxiv.org/abs/1708.04677)  
- [cloud computing](https://arxiv.org/abs/1707.07452)  

---

# Brief history of machine learning

The importance of statistics:  
- [Welling's commentary](https://www.ics.uci.edu/~welling/publications/papers/WhyMLneedsStatistics.pdf)  
- [Srivastava's discussion](https://www.analyticsvidhya.com/blog/2015/07/difference-machine-learning-statistical-modeling/)  
- [Breiman's take](https://projecteuclid.org/euclid.ss/1009213726)  

Seek "actionable insight":  
- ["actionable insight"](https://www.techopedia.com/definition/31721/actionable-insight)  

---

# Supervised machine learning

Selecting a machine learning algorithm depends on the characteristics of the problem being investigated - there is no "best" method applicable to all cases. Machine learning is generally divided into three broad classes of learning: [supervised](https://en.wikipedia.org/wiki/Supervised_learning), [unsupervised](https://en.wikipedia.org/wiki/Unsupervised_learning), and [reinforcement](https://en.wikipedia.org/wiki/Reinforcement_learning). In this workshop we will focus on classification, although a simple regression example is provided as a bonus challenge. 

The syntax for supervised machine learning algorithms can be thought of like this:  

Y ~ X~1~ + X~2~ + X~3~… X~n~

Y is the dependent/response/target/outcome variable  
X are the independent/input/predictor/feature variables  

Supervised machine learning methods learn a target function `\(f\)` that best maps X to Y based on a set of [training data](https://en.wikipedia.org/wiki/Training,_test,_and_validation_sets). 

---

# Supervised machine learning

Our function would look like this: `\(y = f(X) + \epsilon\)`, where `\(f\)` is some function that relates our X predictor variables to Y in an unknown way thus we must estimate it. Epsilon `\(\epsilon\)` is the random error, is independent of X, and averages to zero. Therefore, we can predict Y using `\(\hat{y} = \hat{f}(X)\)` for new data (call the test dataset) and evaluate how well the algorithm learned the target function when introduced to new data.  

**How to define machine learning? (revisited)**  
More specifically, we can think of machine learning as a bunch of methods to estimate `\(f\)`!  

---

# Classification or regression?

**Classification** is used when the Y outcome variable is categorical/discrete. Binary examples generally refer to a yes/no situation where a 1 is prediction of the "yes" category and 0 as the "no". Classification models the probability that the outcome variable is 1 based on the covariates: `\(Pr(Y = 1 | X)\)`. This can be extended to multi-level classification as well.  

**Regression** is used when the target Y outcome variable is continuous. Regression models the conditional expectation (conditional mean) of the outcome variable given the covariates: `\(E(Y | X)\)`. See the bonus challenge for a regression example.  

---

# Data preprocessing

A longstanding first step is to split a dataset into **"training"** and **"test"** subsets. A training sample usually consists of a majority of the original dataset so that an algorithm can learn the model. The remaining portion of the dataset is designated to the test sample to evaluate model performance on data the model has not yet seen. **Missing data should be handled** before the splitting process commences.  

---

# Model performance

**Performance metrics** are used to see how well a model predicts a specified outcome on training and test datasets.  

A model that performs poorly on the training dataset is **underfit** because it is not able to discern relationships between the X and Y variables.  

A model that performs well on the training dataset but poorly on the test dataset is said to be **overfit** because the model performed worse than expected when given new data. To some extent the patterns found in the training data may have been random noise and therefore, by random chance, are different in the test data.  

---

# Common performance metrics

- Accuracy  
- Mean squared error  
- Sensitivity and specificity  
- Area under the ROC curve (AUC)  

---

class: left

# Workshop goals

### 1) Learn the basics of using five machine learning algorithms in R:  

- lasso (prefaced by an ordinary least squares regression refresher)
- decision tree  
- random forest  
- boosting  
- SuperLearner  
  
### 2) Vizualize important information:  
- decision trees  
- random forest variable importance  
- AUC from different boosting models  
- SuperLearner cross-validated risk   

---

# Workshop goals

### 3) Examine the performance of these models  
- lasso penalization coefficient, (root) mean-squared error
- mean decrease accuracy
- accuracy
- AUC
- cross-validated risk

### 4) Simultaneously compare multiple algorithms in an ensemble  
- compare various tunings of multiple algorithms at once
- weighted ensemble average
- discrete winner

---

# Specific workshop goals

Use the  `PimaIndiansDiabetes2` dataset from the [`mlbench` package](https://cran.r-project.org/web/packages/mlbench/mlbench.pdf) to investigate the following questions:  

1) **Lasso regression example:** How well can a person's age be predicted using the other variables?  

2) **Binary classification examples:** How reliably can different machine learning algorithms predict a person's diabetes status using the other variables?  

What are these other variables? Load the data and find out! Open "2-preprocessing.Rmd" to get started. 

---

# What is deep learning? (short)

### - A subfield of machine learning that utilizes multi-layered artificial neural networks for modelling and prediction of data. 

### - These models can be applied to numeric, categorical, image, text, audio, and time-series data. 

### - For images, neural networks import the image as a matrix of pixel values. Therefore it is useful to think of images as giant matrices! 

---

class: center

# What is an artificial neural network? 

"Perceptron"
&lt;img src="slide_img/ann.png" width="600px" style="display: block; margin: auto;" /&gt;

---

# Key concepts
.pull-left[
1) Images are preprocessed 
- Size
- Tensor reshape  
- Image color profile

2) Training and validation sets are split
- Part 1: MNIST 
  - Train = 60000, val = 10000
- Dog-human 
  - Train = 600, val = 100

3) Define data characteristics
- Batch size
- Number of training and validation samples  
- Epochs
]

.pull-right[
4) Define the network
- Number of units in input, hidden, and output layers. 
- Activation functions
- Dropout

5) Compile:  
- Loss function  
- Optimizer  
  - Learning rate
- Performance metric  

6) Train the model

7) Evaluate
]

---

class: center

# What is an activation function?

&lt;img src="slide_img/actfun.png" width="500px" style="display: block; margin: auto;" /&gt;

---

class: center

# What makes a neural network "deep"?

&lt;img src="slide_img/deep.png" width="700px" style="display: block; margin: auto;" /&gt;

---

# How does it work? 

#### 1. Supervised neural networks require "features", or an **input layer** of data that is used to produce our estimated **output layer**, or estimated classification of an actual image.  

#### 2. This input layer is initialized with a vector of randomized weights and a bias term of "1" is added. 

#### 3. The products of these input features and their weights are summed and  transformed. This summed value is passed through an activation function to determine the threshold if our prediction should be classified as a 0 or a 1. A learning rate is also be specified. 

---

# How does it work? 

#### 4. Error is computed as target value minus our estimation. The weights delta metric (how much the errors should change) is calculated as the error times the slope of the point on the activation function times the vector of input features. 

#### 5. The vector of original weights is added to the vector of updated weights and are "backpropagated" (used as the recycled input) and passed through the model for another epoch.  

#### 6. With deep networks, the process also takes places between **hidden layers**, or areas of nonlinear transformations connected only to the layers before and after them. They are referred to as "hidden" because they are not show as the final output. 

---

# Part 1 functions

#### `data_mnist()`
#### `array_reshape()`
#### `to_categorical()`
#### `layer_dense()`
#### `layer_dropout()`
#### `compile()`
#### `fit()`
#### `evaluate()`

---

# Part 1: `data_mnist`
[Load the MNIST dataset](http://yann.lecun.com/exdb/mnist/) using the built-in function `mnist = dataset_mnist()` and assign the training and test X and Y variables: 

#### Training set

```r
x_train = mnist$train$x  
y_train = mnist$train$y
```

#### Validation (test) set

```r
x_test = mnist$test$x  
y_test = mnist$test$y
```

#### Note the array dimension differences

```r
str(mnist$train$x)
str(x_train)
```
---

# Part 1: `array_reshape`
This function converts our 3D tensor (samples, height, width) to a 2D one (samples, input shape). 


```r
x_train = array_reshape(x_train, c(nrow(x_train), height * width))

x_test = array_reshape(x_test, c(nrow(x_test), height * width))
```

#### Check new dimensions (2D)

```r
str(x_train)

str(x_test) 
```

#### Gray rescale:  

```r
x_train = x_train / 255

x_test = x_test / 255
```

---

# Part 1: `to_categorical`
Convert integer vector to binary class matrix. Specify the train and test labels and the number of Y classes (10, the numbers 0 thru 9).  


```r
y_train = to_categorical(y_train, 10) 

y_test = to_categorical(y_test, 10)
```

---

# Part 1: `keras_model_sequential`
This function allows us to build a linear stack of layers. We will make a model with one input layer, one hidden layer, and one output layer. We need to define:  
- The number of units in the layer.  
- Activation function: how to determine the output of a node (e.g., sigmoid, relu, tanh).  
- Input shape: image dimensions  
- Dropout: a regularization method for preventing overfitting by reducing the number of units in a layer during a pass/epoch.  
---

# Part 1


```r
model = keras_model_sequential()

model %&gt;%
  
*# (INPUT LAYER + HIDDEN LAYER)
  layer_dense(units = 256, activation = 'relu', input_shape = c(784)) %&gt;%
  
  layer_dropout(rate = 0.4) %&gt;% 
  
*# (HIDDEN LAYER) 
  layer_dense(units = 128, activation = 'relu') %&gt;%
  
  layer_dropout(rate = 0.3) %&gt;%
  
*# (OUTPUT LAYER)
  layer_dense(units = 10, activation = 'softmax')

summary(model)
```

---

# Part 1: `compile` 
Here we can define the performance evaluation metrics to be incorporated into the model.  

**Loss function:** measures the difference in distance scores between predicted and actual outcomes (i.e., a residual); we hope to minimize this value.  

**Optimizer function:** responsible for the backpropagation algorithm; defines how the loss function's gradient will update the weights. Also associated with the optimizer is the "learning rate", or by _how much_ should we update these weights?  
  
**Performance metric:** We can just choose accuracy since we are tasked with a multinomial classification problem of ten handwritten digits.  


```r
model %&gt;%
  compile(loss = 'categorical_crossentropy',
          optimizer = optimizer_rmsprop(lr = 0.001),
          metrics = c('accuracy'))`
```
---

# Part 1: `fit`
We are almost ready to train the model, but we first need to provide a few more specifications. 

- X and Y training data   
- The number of epochs (full passes) where the weight gradients will be computed to update the weights. 
- Batch size: number of samples from the training set to be used in a single epoch. An "iteration" is the number of batches necessary to complete one epoch, where gradients are computed relative to the loss of the individual batch. 
- Validation split: percentage of data to be used as the validation set (0.2 means last 20%).  


```r
history = model %&gt;%
  fit(x_train, y_train, epochs = 30, batch_size = 128, validation_split = 0.2)
```

---
# Part 1: `evaluate`
Predict on the test set!  


```r
model %&gt;% evaluate(x_test, y_test)
```

---

# Part 1 Challenge
### Write down the steps you followed to run this model from start to finish. What does each part do? 

---

# Part 2 functions
#### `list_files`  
#### `image_data_generator`  
#### `flow_from_directory`  
#### `keras_model_sequential`  
#### `layer_flatten`, `layer_dense`, `layer_dropout`  
#### `compile`  
#### `fit_generator`  
#### `evaluate_generator`  

---

Part 2  

Now that you are warmed up, let's see what Keras can do when we have our own folders of images. Here we have two folders of [dogs](http://vision.stanford.edu/aditya86/ImageNetDogs/) and [people](http://pascal.inrialpes.fr/data/human/), located in training and validation directories. 


```r
train_path = "data-raw/dog-human/TRAIN"

val_path = "data-raw/dog-human/VAL"
```

## `list_files`
Listing the files prints the full file paths of our images.  


```r
train_images = list.files(train_path, full.names = TRUE, recursive = TRUE)

val_images = list.files(val_path, full.names = TRUE, recursive = TRUE)
```

---

Part 2  

We can fit the model like before, by defining the number of units in each layer, the activation function, the input size, etc. However we should also specify: 
- Image width and height  
- Batch size  
- Number of training samples  
- Number of validation samples  
- Epochs  

---

# Part 2: `image_data_generator`
This function [performs data augmentation](https://keras.rstudio.com/reference/image_data_generator.html). It contains many tunable parameters, but we will just use the rescale to the gray channel. 


```r
train_datagen = keras::image_data_generator(rescale = 1/255)  

val_datagen = keras::image_data_generator(rescale = 1/255)
```
---

# Part 2: `flow_from_directory`
Here we can configure the training model. You will need to define: 
- The file paths to the training and test data  
- Image shape  
- Batch size  
- Output class: binary
- Color profile: grayscale

&gt; Remember this is a Pythonic "method"! 

---

# Part 2: `keras_model_sequential`

Fit the model! 

Same as Part 1, except we need to first flatten the three dimensional array to one dimension:  


```r
layer_flatten(input_shape = c(img_width, img_height, 1))
```

The `compile` step (loss function, activation function, and performance metric) is also the same as Part 1. 

---

# Part 2: `fit_generator`

This fits the training model configuration (using `flow_from_directory`)  
- Steps per epoch: the number of batch runs to complete one full iteration of training.  
- Epochs: 
- Validation data: the validation model 
- Validation steps: the number of batch runs needed to complete one full iteration of validation.  


```r
history = model %&gt;%
  fit_generator(train_gen, steps_per_epoch = as.integer(num_train_samples / batch_size),
  epochs = epochs, validation_data = val_gen, validation_steps = as.integer(num_validation_samples / batch_size))
```

---

# Part 2: `evaluate_generator`
Predict on the validation data! 


```r
model %&gt;% evaluate_generator(generator = val_gen, steps = 10)
```

---

# Part 2 Challenge
How can we improve performance of this neural network? [Read this excellent post](https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607) to learn 37 different ways. Try tuning some of our model (hypter)parameters in the above code to see if you can improve model performance. 

---

# Acknowledgements  
Allaire JJ. 2017. [keras for R](https://blog.rstudio.com/2017/09/05/keras-for-r/)

Allaire JJ, Chollet F. 2018. keras: R interface to 'Keras'.
- https://keras.rstudio.com

- https://cran.r-project.org/web/packages/keras/vignettes/getting_started.html  

Chollet F., Allaire JJ. 2018. [Deep Learning in R](https://www.manning.com/books/deep-learning-with-r). Manning Publications  
ISBN 9781617295546.  

Dancho M. 2018. [Deep learning with keras to predict customer churn.](https://blogs.rstudio.com/tensorflow/posts/2018-01-11-keras-customer-churn/)

[Getting started](https://cran.r-project.org/web/packages/keras/vignettes/getting_started.html) with keras: Overview vignette. 

Lakhani, P. 2018. [Hello_World_Deep_Learning.](https://github.com/paras42/Hello_World_Deep_Learning)

[View the resouces listed](https://github.com/dlab-berkeley/Deep-Learning-in-R/blob/master/README.md) in this repository's readme file. 

Images [borrowed from Qingkai Kong's ANNs in Python workshop.](https://github.com/qingkaikong/20181129_ANN_basics_DLab)
    </textarea>
<script src="libs/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"slideNumberFormat": "<div class=\"progress-bar-container\">   <div class=\"progress-bar\" style=\"width: calc(%current% / %total% * 100%);\">   </div> </div>` "
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
