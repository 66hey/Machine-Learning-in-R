# XGBoost

## Overview

from [Freund Y, Schapire RE. 1999. A short introduction to boosting. Journal of Japanese Society for Artificial Intelligence 14:771-780](https://cseweb.ucsd.edu/~yfreund/papers/IntroToBoosting.pdf):  

"Boosting is a general method for improving the accuracy of any given learning algorithm" and evolved from AdaBoost and PAC learning (p. 1-2). Gradient boosted machines are ensembles decision tree methods of "weak" trees that are just slightly more accurate than random guessing. These are then "boosted" into "strong" learners. That is, the models don't have to be accurate over the entire feature space.  

The model first tries to predict each value in a dataset - the cases that can be predicted easily are _downweighted_ so that the algorithm does not try as hard to predict them.  

However, the cases that the model has difficulty predicting are _upweighted_ so that the model more assertively tries to predict them. This continues for multiple "boosting iterations", with a training-based performance measure produced at each iteration. This method can drive down generalization error (p. 5). 

Rather than testing only a single model at a time, it is useful to tune the parameters of that single model against multiple versions. Also, bootstrap is the default, but we want cross-validation.  

First create two objects - `cv_control` and `xgb_grid`. `xgb_control` will allow us to customize the cross-validation settings, while `xgb_grid` lets us evaluate the model with different settings:
```{r caret_prep}
# Use 5-fold cross-validation with 2 repeats as our evaluation procedure.
# (instead of the default "bootstrap").
cv_control =
  trainControl(method = "repeatedcv",
               number = 5L,
               repeats = 2L,
               # Calculate class probabilities.
               classProbs = TRUE,
               # Indicate that our response variable is binary.
               summaryFunction = twoClassSummary) 

# Ask caret what hyperparameters can be tuned for the xgbTree algorithm.
modelLookup("xgbTree")
getModelInfo("xgbTree")

# More details at https://xgboost.readthedocs.io/en/latest/parameter.html
(xgb_grid = expand.grid(
  # Number of trees to fit, aka boosting iterations
  nrounds = c(50, 250, 800),
  # Depth of the decision tree (how many levels of splits).
	max_depth = c(1L, 3L, 6L), 
  # Learning rate: lower means the ensemble will adapt more slowly.
	eta = c(0.0001, 0.01, 0.2),
  # Make this larger and xgboost will tend to make smaller trees
  gamma = 0,
  colsample_bytree = 1.0,
  subsample = 1.0,
  # Stop splitting a tree if we only have this many obs in a tree node.
	min_child_weight = 10L))
# Other hyperparameters: gamma, column sampling, row sampling

# How many combinations of settings do we end up with?
nrow(xgb_grid)
```

Fit the model. Note that we will now use area under the ROC curve (called "AUC") as our performance metric, which relates the number of true positives (sensitivity) to the number of true negatives (specificity).  

> NOTE: This will take a few minutes to complete! See the .html file for the output.

```{r xgb_fit, cache = TRUE}

# Convert our numeric indicators (1s and 0s) back into factors ("pos" and "neg")
trainlab_factor = factor(ifelse(train_label == 1, "pos", "neg"))
testlab_factor = factor(ifelse(test_label == 1, "pos", "neg"))
table(trainlab_factor, train_label)
table(testlab_factor, test_label)

set.seed(1)

# cbind: caret expects the Y response and X predictors to be part of the same dataframe
model = caret::train(trainlab_factor ~ ., data = cbind(trainlab_factor, train_x), 
             # Use xgboost's tree-based algorithm (i.e. gbm)
             method = "xgbTree",
             # Use "AUC" as our performance metric, which caret incorrectly calls "ROC"
             metric = "ROC",
             # Specify our cross-validation settings.
             trControl = cv_control,
             # Test multiple configurations of the xgboost algorithm.
             tuneGrid = xgb_grid,
             # Hide detailed output (setting to TRUE will print that output).
             verbose = FALSE)

# See how long this algorithm took to complete.
model$times 

# Review model summary table.
model

# Extract the hyperparameters with the best performance.
model$bestTune

# And the corresponding performance metrics
model$results[as.integer(rownames(model$bestTune)), ]

# Plot the performance across all hyperparameter combinations.
ggplot(model) + theme_minimal() + ggtitle("Xgboost hyperparameter comparison") 

# Show variable importance (text).
caret::varImp(model)

# remotes::install_github("koalaverse/vip")

# This version uses the complex caret object
vip::vip(model) + theme_minimal()

# This version operates on the xgboost model within the caret object
vip::vip(model$finalModel) + theme_minimal()

# Generate predicted labels.
predicted_labels = predict(model, test_x)
table(test_label, predicted_labels)

# Generate class probabilities.
pred_probs = predict(model, test_x, type = "prob")
head(pred_probs)

# View final model
(cm = confusionMatrix(predicted_labels, testlab_factor))

# Define ROC characteristics
(rocCurve = pROC::roc(response = testlab_factor,
                predictor = pred_probs[, "pos"],
                levels = rev(levels(testlab_factor)),
                auc = TRUE, ci = TRUE))

# Plot ROC curve with optimal threshold.
plot(rocCurve, print.thres = "best", main = "XGBoost on test set", col = "blue") 

```

##### Challenge 4
**Big question 4:** What are some defining characteristics of the algorithms we have covered in these five exercises?