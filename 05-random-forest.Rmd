# Random Forests

## Overview

The random forest algorithm seeks to improve on the performance of a single decision tree by taking the average of many trees. Thus, a random forest can be viewed as an **ensemble** method, or model averaging approach. The algorithm was invented by UC Berkeley's own Leo Breiman in 2001, who was also a co-creator of decision trees (see his [1984 CART book](https://www.amazon.com/Classification-Regression-Wadsworth-Statistics-Probability/dp/0412048418)).  

Random forests are an extension of **bagging**, in which multiple samples of the original data are drawn with replacement (aka "bootstrap samples"). An algorithm is fit separately to each sample, then the average of those estimates is used for prediction. While bagging can be used by any algorithm, random forest uses decision trees as its base learner. Random forests add another level of randomness by also randomly sampling the features (or covariates) at each split in each decision tree. This makes the decision trees use different covariates and therefore be more unique. As a result, the average of these trees tends to be more accurate overall.

Fit a random forest model that predicts the number of people with diabetes using the other variables as our X predictors. If our Y variable is a factor, randomForest will by default perform classification; if it is numeric/integer regression will be performed and if it is omitted it will become unsupervised! 
```{r rf_fit}
set.seed(1)
(rf1 = randomForest::randomForest(as.factor(train_label) ~ ., 
                   data = train_x, 
                   # Number of trees
                   ntree = 500, 
                   # Number of variables randomly sampled as candidates at each split.
                   mtry = 2, 
                   # We want the importance of predictors to be assessed.
                   importance = TRUE))
```

The "OOB estimate of error rate" shows us how accurate our model is. $accuracy = 1 - error rate$. OOB stands for "out of bag" - and bag is short for "bootstrap aggregation". So OOB estimates performance by comparing the predicted outcome value to the actual value across all trees using only the observations that were not part of the training data for that tree.

We can examine the relative variable importance in table and graph form. Random Forest estimates variable importance by separately examining each variable and estimating how much the model's accuracy drops when that variable's values are randomly shuffled (permuted). The shuffling temporarily removes any relationship between that covariate's value and the outcome. If a variable is important then the model's accuracy will suffer a large drop when it is randomly shuffled. But if the model's accuracy doesn't change it means the variable is not important to the model - e.g. maybe it was never even chosen as a split in any of the decision trees.
```{r rf_varImpPlot}
varImpPlot(rf1)

# Raw data
rf1$importance
```

You can read up on the [gini coefficient](https://en.wikipedia.org/wiki/Gini_coefficient) if interested. It's basically a measure of diversity or dispersion - a higher gini means the model is classifying better. The gini version does not randomly shuffle the variable values.  

Now, the goal is to see how the model performs on the test dataset:
```{r}
# This will predict the outcome class.
predicted_label = predict(rf1, newdata = test_x)
table(predicted_label, test_label)
```

Check the accuracy of the test set:
```{r prob_hist}
mean(predicted_label == test_label) 

# We can also generated probability predictions, which are more granular.
predicted_prob = as.data.frame(predict(rf1, newdata = test_x, type = "prob"))
colnames(predicted_prob) = c("no", "yes")
summary(predicted_prob)
ggplot(predicted_prob, aes(x = yes)) + geom_histogram() + theme_minimal()

# devtools::install_github("ck37/ck37r")

# Review number of terminal nodes (aka "leaves") across the decision trees.
summary(ck37r::rf_count_terminal_nodes(rf1))
```

How did it do? Are the accuracies for the training and test sets similar?  

**Big question 3:** Why is the random forest algorithm preferred to a single decision tree or bagged trees?

##### Challenge 3

1. Try a few other values of mtry - can you find one that has improved performance?
2. Maxnodes is another tuning parameter for randomForest - does changing it improve your performance?
3. Use the iris dataset to perform classification on the "Species" variable. What are you noticing about model fits between the pidd and iris datasets? 