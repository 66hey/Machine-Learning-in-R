# Decision Trees

## Clear your workspace

Delete the regression variables from your global environment. We will be working with the classification variables from now on. 

```{r}
rm(list = ls())
```

## Load packages

```{r load_packages}
library(ggplot2)
library(rpart)
library(rpart.plot)
```

## Load data 

Load the training and test x and y variables we created in 02-preprocessing.Rmd for this *classification* task. 

```{r setup_data}
# Objects: task_reg, task_class
load("data/preprocessed.RData")

# Pull out classification preprocessed data for easier analysis.
train_x = task_class$data[task_class$train_rows, task_class$covariates]
train_y = task_class$data[task_class$train_rows, task_class$outcome]

test_x = task_class$data[-task_class$train_rows, task_class$covariates]
test_y = task_class$data[-task_class$train_rows, task_class$outcome]
```

## Overview

Decision trees are recursive partitioning methods that divide the predictor spaces into simpler regions and can be visualized in a tree-like structure. They attempt to classify data by dividing it into subsets according to a Y output variable and based on some predictors.  

Let's see how a decision tree classifies if a person suffers from heart disease (`target` = 1) or not (`target` = 0).

## Fit Model

```{r}
set.seed(3)
tree = rpart::rpart(train_y ~ ., data = train_x,
             # Use method = "anova" for a continuous outcome.
             method = "class",
             # Can use "gini" for gini coefficient.
             parms = list(split = "information")) 

# Here is the text-based display of the decision tree. Yikes!  :^( 
print(tree)
```

Although interpreting the text can be intimidating, a decision tree's main strength is its tree-like plot, which is much easier to interpret.

## Investigate Results

```{r plot_tree}
rpart.plot::rpart.plot(tree) 
```

We can also look inside of `tree` to see what we can unpack. "variable.importance" is one we should check out! 

```{r}
names(tree)
tree$variable.importance
```

Plot variable importance
```{r}
# Turn the tree$variable.importance vector into a dataframe
tree_varimp = data.frame(tree$variable.importance) # coerce to dataframe
tree_varimp$x = rownames(tree_varimp) # add rownames as their own column
tree_varimp = tree_varimp[, c(2,1)] # reorder columns
rownames(tree_varimp) = NULL # reset row names
names(tree_varimp) = c("Variable", "Importance") # rename columns
tree_varimp

# Plot
ggplot(tree_varimp, aes(x = reorder(Variable, Importance), 
                        y = Importance)) + 
  geom_bar(stat = "identity") + 
  theme_bw() + coord_flip() + xlab("")
```

In decision trees the main hyperparameter (configuration setting) is the **complexity parameter** (CP), but the name is a little counterintuitive; a high CP results in a simple decision tree with few splits, whereas a low CP results in a larger decision tree with many splits.  

`rpart` uses cross-validation internally to estimate the accuracy at various CP settings. We can review those to see what setting seems best.  

Print the results for various CP settings - we want the one with the lowest "xerror". We can also plot the performance estimates for different CP settings. 
```{r plotcp_tree}
# Show estimated error rate at different complexity parameter settings.
printcp(tree)

# Plot those estimated error rates.
plotcp(tree)

# Trees of similar sizes might appear to be tied for lowest "xerror", but a tree with fewer splits might be easier to interpret. However, a tree with 6 and 7 splits have lower relative errors. 
tree_pruned2 = prune(tree, cp = 0.028986) # 2 splits
tree_pruned6 = prune(tree, cp = 0.010870) # 6 splits
```

Print detailed results, variable importance, and summary of splits.

```{r}
summary(tree_pruned2) 
rpart.plot(tree_pruned2)
```
```{r}
summary(tree_pruned6) 
rpart.plot(tree_pruned6)
```

You can also get more fine-grained control by checking out the "control" argument inside the rpart function. Type `?rpart` to learn more.  

## Challenge 2
Open Challenge 2 in the "Challenges" folder. 
