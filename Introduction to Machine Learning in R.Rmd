---
title: "Introduction to Machine Learning in R"
author: "Evan Muzzall and Chris Kennedy"
date: "2/5/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#1. Pre-introduction
#####1. Pre-introduction - package installation
Be sure you have installed and libraried the following packages:
```{r, eval=FALSE}
install.packages(c("car", "caret","class","gbm", "ggplot2", "gmodels", "pROC", "randomForest","rpart","SuperLearner"), dependencies=TRUE)
library(car)
library(caret)
library(class)
library(gbm)
library(ggplot2)
library(gmodels)
library(pROC)
library(randomForest)
library(rpart)
library(SuperLearner)
```

#2. Introduction to machine learning - a brief history
Machine learning evolved from scientific pursuits in computational/information theory, artificial intelligence, and pattern recognition. In the modern sense, it arguably dates to [Blaise Pascal's calculator](http://history-computer.com/MechanicalCalculators/Pioneers/Pascal.html). See [Welling's commentary](https://www.ics.uci.edu/~welling/publications/papers/WhyMLneedsStatistics.pdf) about the importance of statistics in machine learning. 

However, contemporary machine learning is often attributed to the ideas of people such as Alan Turing, Frank Rosenblatt, and Arthur Samuel.  

The "Turing Test" sought to examine a computer's ability to use natural language like a human could. A human and computer would enter text onto a screen and if a judge could not determine which was the human and which was the computer, the computer would pass the Turing Test.  

[Alan Turing biography](http://www.turing.org.uk/publications/dnb.html)  
[Turing AM. 1950. Computing Machinery and Intelligence. _Mind_ 59:433-460](http://www.jstor.org/stable/pdf/2251299.pdf)  
[Saygin AP, Cicekli I, Akman V. 2003 Turing Test: 50 Years Later. The Turing Test, pp. 23-78. Springer Netherlands](http://cogprints.org/1925/2/tt50.ps)  

In the 1950s, Arthur Samuel demonstrated how a computer program could learn the rules of checkers, begin to think ahead, and beat a human checkers player in a relatively short period of time. In his seminal 1959 paper, Arthur Samuel defined "machine learning" as the ability for a machine to learn without having to be programmed by a human. That is, it could adapt to relatively simple rules for increasingly strategized endgames.  

[Arthur Samuel biography](http://infolab.stanford.edu/pub/voy/museum/samuel.html)  
[Samuels, A. 1959. Some Studies in Machine Learning Using the Game of Checkers. _IBM Journal of Research and Development_](http://ucelinks.cdlib.org:8888/sfx_local?sid=google&auinit=AL&aulast=Samuel&atitle=Some+studies+in+machine+learning+using+the+game+of+checkers&id=doi:10.1147/rd.33.0210&title=IBM+Journal+of+Research+and+Development&volume=3&issue=3&date=1959&spage=210&issn=0018-8646)  

In 1958, Frank Rosenblatt developed the perceptron, or the first neural network for study of the brain. 

[Frank Rosenblatt biography](http://csis.pace.edu/~ctappert/srd2011/rosenblatt-contributions.htm)
[Rosenblatt F. 1958. The perceptron: A probabilistic model for information storage and organization in the brain. _Psychological Review_ 65:386-408](http://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf)  

Machine learning in the 1960s, 1970s, and 1980s saw a variety of developments in algorithm creation and application, pattern recognition, and explanation based learning. The 1990s witnessed a data and application-driven approach, and the 21st century has focused into adaptation and abstraction as well as reinforcement learning and virtual reality.  

Once thought to be exclusive to computer science, the modern strength of machine learning lies in its breadth of application. It is now used to provide ["actionable insight"](https://www.techopedia.com/definition/31721/actionable-insight) for research, problem solving, and decision making in technological industries, business, language, the social and biological sciences, and humanities.  

[Dig into Machine Learning with this interactive chart to learn more](http://sge.wonderville.ca/machinelearning/history/history.html)  

#3. Machine learning algorithms
Machine learning algorithm selection depends on the nature of the problem being investigated. For the purposes of this workshop, algorithms are generally divided into three types: supervised, unspervised, and reinforcement learners. We will focus on supervised learning methods for this workshop. Visit the below links for an introduction to unsupervised and reinforcement learning:

[Ghahramani Z. 2004. Unsupervised Learning. In _Advanced Lectures on Machine Learning LNIAI 3176,_ edited by Bousquet O, Raetsch G, von Luxburg, pp. 72-112. New York: Springer Verlag](http://www.inf.ed.ac.uk/teaching/courses/pmr/docs/ul.pdf)

[Sutton RS, Barto AG. 2012. Reinforcement Learning: An Introduction. Cambridge, MA: The MIT Press](http://people.inf.elte.hu/lorincz/Files/RL_2006/SuttonBook.pdf))  

Supervised machine learning algorithms learn a target funciton _f_ that best maps to a response variable (Y) based on predictor variable(s) (X). It figures how to estimate Y based on new data for X. However, these predictions are influenced by three types of error: bias, variance, and irreducible error. See [James et al., pp. 18, 33-37.](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Sixth%20Printing.pdf) for definitions.  

This is often done by first splitting a dataset into "training" and "test" datasets. A training dataset usually consists of a majority portion of the original dataset to LEARN THE MODEL. The remaining portion of the dataset is then designated to the test dataset, WHICH IS USED AS A HOLDOUT FOR PERFORMANCE ON PREDICTED VALUES.

It is common practice to "train" a model to the "training dataset" so that it can learn the target function _f_. Then, the model is fitted to the "test dataset" to see how it performs. The accuracy of the prediction on the test dataset is then gauged. 

Classification and regression are examples of supervised learning.  (move this up)

Unsupervised machine learning algorithms only utilize predictor variables (X) and no Y response variable. The goal is to instead discern the underlying structure of the data. Clustering and ordination techniques are examples of unsupervised learning.  (move this up)

> *A short way to remember this*: if you have a Y response variable that you are trying to predict using X predictor variables, you would probably consider a Supervised learning method. If you lack a Y response variable and only seek to investigate the data distribution found with X input variables, you would likely choose an Unsupervised learning method.  (move this up)

See Chapters 1 and 2 from [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Sixth%20Printing.pdf) for an introduction to Supervised learning, and see Chapter 10 for an overview of Unsupervised learning.  

#####3. Machine learning algorithms - classification or regression?
In supervised machine learning, classification and regression are the most common approaches. Classification should be used when your target Y variable is discrete, such as...

Regression should be used when the target Y variable is continuous, such as ...

#####3. Machine learning algorithms - research design
ADD MORE VERBIAGE ABOUT WHAT THE PRACTICAL APPLICATION OF THESE AUTHORS.

Before performing machine learning in R, one must consider the nature of the problem being investigated. Some supervised examples include:  

In business, a real estate agent might want to predict the price of houses in a neighborhood based on their different individual features, proximity to schools, and tax and crime rates of the area.  

[Ahmed EH, Moustafa MN. 2016. House price estimation from visual and textual features Proceedings of the 8th International Joint Conference on Computational Intelligence doi:10.5220/0006040700620068](https://arxiv.org/pdf/1609.08399.pdf)

In the biological sciences, a climate scientist might use past and present weather data to try and predict future weather patterns. 

[Feng QY, Vasile R, Segond M, Gozolchiani A, Wang Y, Abel M, Havlin S, Bunde A, Dijkstra HA. 2016. ClimateLearn: A machine-learning approach for climate prediction using network measures. _Geoscientific Model Development_ doi:10.5194/gmd-2015-273](http://www.geosci-model-dev-discuss.net/gmd-2015-273/gmd-2015-273.pdf)

In the social sciences, a researcher might choose to predict voter turnout based on the number of people currently registered to vote and preliminary polling numbers.  

[Nickerson DW, Rogers T. 2014. Political Campaigns and Big Data. Journal of Economic Perspectives 28:51-74](http://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.28.2.51)

ADD PUBLIC HEALTH EXAMPLE

In the humanities, a researcher might want to reconstruct the relationships of prominent figures in historical texts. (REWORD)

[Argamon S, Olsen M. 2009. Words, Patterns and Documents: Experiments in Machine Learning and Text Analysis. Digital Humanities Quarterly 3(2)](http://www.digitalhumanities.org/dhq/vol/3/2/000041/000041.html)
[Horton R, Morrissey R, Olsen M, Roe G, Voyer R. 2009. Mining Eighteenth Century Ontologies: Machine Learning and Knowledge Classification in the Encyclop√©die. Digital Humanities Quarterly 3(2)](http://www.digitalhumanities.org/dhq/vol/3/2/000044/000044.html)

#####3. Machine learning algorithms - performance metrics
Once an appropriate algorithm is designed and implemented, its performance on new data should be gauged. If an algorithm performs poorly on a training dataset, it could be said to be _underfit_ because it could not discern the relationships between the X and Y variables. A model is said to be _overfit_ if it performs well on the training dataset but poorly on the test dataset because it was not able to learn the mapping funciton well enough to be able to generalize to new data.  

Performance metrics are generally measured via accuracy, true positive/true negative rates, area under the ROC curve, and cross-validation.  

[Jason Brownlee - How to Evaluate Machine Learning Algorithms](http://machinelearningmastery.com/how-to-evaluate-machine-learning-algorithms/)  
[Jason Brownlee - Assessing and Comparing Classifier Performance with ROC Curves](http://machinelearningmastery.com/assessing-comparing-classifier-performance-roc-curves-2/)  
[James - ROC, pp.147-149](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Sixth%20Printing.pdf)

[R-bloggers - cross validation example](https://www.r-bloggers.com/cross-validation-for-predictive-analytics-using-r/)

[Rob J. Hyndman - CV](http://robjhyndman.com/hyndsight/crossvalidation/)

#4. Load and split the Mroz dataset
We will use the "Mroz" dataset from the ["car" R package](https://cran.r-project.org/web/packages/car/car.pdf). The Mroz dataset contains informaiton about women's participation in the workforce in 1975. 
```{r}
library(car)
data(Mroz)
?Mroz
str(Mroz)
```

The original research article can be found here:
[Mroz TA. 1987. The sensitivity of an empirical model of married women's hours of work to economic and statistical assumptions. _Econometrica_ 55:765-799](http://eml.berkeley.edu/~cle/e250a_f13/mroz-paper.pdf)

Now, let's make copies of the dataset because we need to specify the format of some of the imputs for some of the examples:
```{r}
Mroz_knn <- Mroz # k-Nearest Neighbors example
Mroz_reg <- Mroz # regression example
```

#5. k-Nearest Neighbor (kNN)
The k-Nearest Neighbor algorithm is a good starting point for introducing machine learning because it makes no assumptions about the underlying distribution of the data. kNN uses the characteristics of the points around it to determine how to classify the point in question. For this example, point distances are measured via Euclidean distance. 

"k" is the number of neighbors used to classify the point. Choosing a proper "k" is integral to finding the best-performing model. *Large k-values* might be bad because then the largest sample size would win because it would get the most votes. *Small k-values* are bad because results will become sensitive to the influence of outliers while disregarding the influences of other neighboring points.  

#####5. k-Nearest Neighbor (kNN)
(INSERT PROBLEM STATEMENT - we want to predict "lpf" or women's participation in the workforce)

First we must preprocess the data so that factor types are coerced to integer type:
```{r}
str(Mroz_knn)
Mroz_knn$wc <- ifelse(Mroz_knn$wc=="no", 1L, 0L)
Mroz_knn$hc <- ifelse(Mroz_knn$hc=="no", 1L, 0L)
str(Mroz_knn)
```
`L` is used to coerce numeric data to integer type. 

Now, let's split the dataset 70% will be assigned to the training dataset, while the remaining 30% will be ascribed to the test dataset. 
```{r}
library(caret)
set.seed(1)
split_knn <- createDataPartition(Mroz_knn$lfp, p=0.70, list=FALSE)
train_knn <- Mroz_knn[split_knn, ] # partition training dataset
test_knn <- Mroz_knn[-split_knn, ] # partition test dataset

train_labels_knn <- train_knn[,1] # partition training Y variable vector
test_labels_knn <- test_knn[,1] # partition test Y variable vector 
```

We now have training and test datasets and vector labels to use below.  

Different methods exist for choosing a start point for "k". Let's use the square root of the number of rows in the training datset:
```{r}
round(sqrt(nrow(train_knn)),2) # 22.98
```

Fit the model. Our goal is to predict the classification accuracy of our Y variable (no/yes whether or not a woman participated in the workforce) based on the other X predictor variables:
```{r}
library(class)
set.seed(1)
Mroz_pred_knn <- knn(train=train_knn[,-1], test=test_knn[,-1], cl=train_labels_knn, k=23, prob=TRUE)
```

Let's examine a contingency table and use accuracy as our performance metric to see how well the kNN algorithm predicted "No" and "Yes":
```{r}
library(gmodels)
CrossTable(x=test_labels_knn, y=Mroz_pred_knn, 
           prop.chisq=FALSE,
           prop.r=FALSE,
           prop.c=FALSE,
           prop.t=FALSE)
```
How did it do? Although accuracy was the performance metric used in this example, others could be applied. Furthermore, kNN regression can also be implemented.  

ADD A PART ABOUT IMPROVING PERFORMANCE METRICS HERE:

###Challenge 1  
What happens to the predictive accuracy when k=2 and k=36?  

#6. Linear regression
Regression is used when the target Y variable is continuous. There are many different forms of regression ranging from simple to very complex. For this example, we will use simple linear regression. 

Thus, we will coerce all factors to numeric data types:
```{r}
str(Mroz_reg)
Mroz_reg$lfp <- ifelse(Mroz_reg$lfp=="no", 1L, 0L)
Mroz_reg$wc <- ifelse(Mroz_reg$wc=="no", 1L, 0L)
Mroz_reg$hc <- ifelse(Mroz_reg$hc=="no", 1L, 0L)
str(Mroz_reg)
```

Now, we can fit a model that attempts to predict the Y variable log wage rate ("lwg") via the other X predictor variables. Mean squred error (MSE) will be our performance metric. MSE measures the difference between observed and expected values, with smaller values tending to reflect greater predictive accuracy. 
```{r}
Mroz_reg_model <- lm(lwg ~ ., data=Mroz_reg) # fit the model

reg_pred <- predict(Mroz_reg_model, Mroz_reg) # calculate predictions

(MSE <- mean((Mroz_reg$lwg - reg_pred)^2)) # calculate MSE 
```

### Challenge 2
What happens to the MSE when we try to predict "age" and "lfp"? 

#6. Decision trees
Decision trees are recursive partitioning methods that attempt to classify data by dividing it into subsets based on other variables. Let's see how a tree-based method classifies women's participation in the workforce using the other variables as predictors:
```{r}
library(rpart)
Mroz_dt <- rpart(lfp ~ ., data=Mroz, method="class")  # note: choose method="anova" for a regression tree

printcp(Mroz_dt) # print the results

plotcp(Mroz_dt) # plot the cross-validated results

summary(Mroz_dt) # print results, variable importance, and summary of splits

par(mar=c(0,0,1,0))
plot(Mroz_dt, uniform=TRUE, main="Mroz classificaiton tree")
text(Mroz_dt, use.n=FALSE, all=TRUE, cex=0.75)
```

Here, each node shows the cutoff for each of the predictors and how it contributed to the split. Pruning methods also exist to avoid overfitting the data. 

### Challenge 3
Make a decision tree that tries to classify something other than "lfp".

#6. Random forests
Random forests are recursive partitioning methods that use multiple decision trees for ensemble predictions for regression and classificaiton. Unlike decision trees, by default results generally do not require pruning and include accuracy and variable importance information. Furthermore, at each random forest tree split, only a small portion of the predictors are used (rather than the full suite).  

The decision tree method did not require us to split the data into training and test sets. Let us split it now! 
```{r}
library(caret)
set.seed(1)
split_rf <- createDataPartition(Mroz$lfp, p=0.70, list=FALSE)
train_rf <- Mroz[split_rf,]
test_rf <- Mroz[-split_rf,]
```

Now, let's fit a model that tries to predict the number of women who participated in the labor force in 1985. 
```{r}
library(randomForest)
set.seed(1)
rf1 <- randomForest(lfp ~ ., 
                    data=train_rf, 
                    ntree=500, # ntree = 
                    mtry=2, # mtry = 
                    importance=TRUE) # importance = 

rf1
```

Check the accuracy of the training set like this:
```{r}
(164+231)/nrow(train_rf) # 0.748
```

We can also examine relative variable importance in table and graph form:
```{r}
rf1$importance
barchart(rf1$importance, main="rf barchart", col="blue", border="black")
dotplot(rf1$importance, main="rf dotplot", col=c(1,4))
```

Now, the goal is to see how the model performs on the test dataset:
```{r}
set.seed(1)
rf_pred <- predict(rf1, newdata=test_rf)
table(rf_pred, test_rf$lfp)
```

Check the accuracy of the test set:
```{r}
(62+100)/nrow(test_rf) # 0.72
```
How did it do? 

# Challenge 4
Select another Y variable to predict and evaluate performance on the train_rf and test_rf. Does your accuracy increase/decrease/stay the same? 

#8. Boosting
From [Freund and Schapire 1999](https://cseweb.ucsd.edu/~yfreund/papers/IntroToBoosting.pdf). 
"Boosting is a general method for improving the accuracy of any given learning algorithm" and originated in the AdaBoost and PAC learning (p. 1-2). Gradient boosted machines are ensembles decision tree methods of "weak" trees that are just slightly more accurate than random guessing which are then "boosted" into "strong" learners. That is, the models don't have to be accurate over the entire feature space.  

The model first tries to predict each value in a dataset - the cases that can be predicted easily are _downweighted_ so that the algorithm does not have to try as hard to predict them.  

However, the cases that the model has difficulty predicting are _upweighted_ so that the model tries more assertively to predict them. This continues for multiple "boosting" iterations. A resample-based performance measure is produced at each iteration. Error is measured on the weak learners so that even performing slightly better than random guessing improves accuracy fast (p.2). This method can drive down generalization error thus preventing overfitting (p. 5). While it is susceptible to noise, it is robust to outlier detection.  

Split the data
```{r}
split_gbm <- createDataPartition(Mroz$lfp, p=0.75, list=FALSE)
train_gbm <- Mroz[split_gbm,]
test_gbm <- Mroz[-split_gbm,]
```

Rather than testing a single model at a time, it is useful to compare tuning parameters within a single model. Bootstrapp is the default resampling method, but we want cross-validation. We also want to specify different tuning parameters. 
```{r}
control <- trainControl(method="repeatedcv",#
	repeats=5, #
	classProbs=TRUE, #
	summaryFunction=twoClassSummary) #

grid <- expand.grid(n.trees=seq(1,2000, by=50), #
	interaction.depth=c(1, 3, 5), #
	shrinkage=c(0.01, 0.05, 0.1), #
	n.minobsinnode=10) #
```

Fit the model:
```{r}
set.seed(1)
gbm1 <- train(lfp ~ ., data=train_gbm,
	method="gbm",
	metric="ROC",
	trControl=control,
	tuneGrid=grid,
	verbose=FALSE)
gbm1$times

gbm1 # produce model summary table

summary(gbm1, las=2) # plot variable relative influence     #######why are axis labels not printing here?

ggplot(gbm1) + theme_bw() + ggtitle("GBM model comparisons") # plot the CV ROC accuracy of the tuned models
```


#9. Ensemble methods
... quick SuperLerner overview
Now with ensemble methods we can compare all of these algorithms simultaneously! 
- kNN, lm, decision tree, random forest, and gbm. 
This is an efficient way to see which method performs the best! 

#####Acknowledgements