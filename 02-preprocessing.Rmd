# Preprocessing

## Load the data

Load the heart disease and iris datasets. 

```{r load_data}
# Load the heart disease dataset.
heart = read.csv("./inbound/heart.csv")
str(heart)
```

## Read background information and variable descriptions  
https://archive.ics.uci.edu/ml/datasets/heart+Disease

```{r}
# Load the iris dataset (built into R)
data(iris)
?iris
str(iris)
```

## Data preprocessing

Data peprocessing is an integral first step in machine learning workflows. Because different algorithms sometimes require the moving parts to be coded in slightly different ways, always make sure you research the algorithm you want to implement so that you properly setup your $y$ and $x$ variables and split your data appropriately. 

One additional preprocessing aspect to consider: datasets that contain factor (categorical) features should typically be expanded out into numeric indicators (this is often referred to as [one-hot encoding](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f). You can do this manually with the `model.matrix` R function. This makes it easier to code a variety of algorithms to a dataset as many algorithms handle factors poorly (decision trees being the main exception). Doing this manually is always good practice. In general however, functions like `lm` will do this for you automatically. More on this below. 

> NOTE: Keep in mind that training/test dataset splitting is common, but not always preferred. We will introduce you to cross-validation in the second half of this workshop where _all_ the data are used and multiple training/testing splits are utilized. 

## Handling missing data

Missing values need to be handled somehow. Listwise deletion (deleting any row with at least one missing value) is common but this method throws out a lot of useful information. Many advocate for mean imputation, but arithmetic means are sensitive to outliers. Still, others advocate for Chained Equation/Bayesian/Expectation Maximization imputation (e.g., the [mice](https://www.jstatsoft.org/article/view/v045i03/v45i03.pdf) and [Amelia II](https://gking.harvard.edu/amelia) R packages). 

K-nearest neighbor imputation can also be useful but median imputation is demonstrated below. First, count the number of missing values across variables in our dataset.  

```{r review_missingness}
colSums(is.na(heart))
```

We have no missing values, so let's introduce a few to the "oldpeak" feature for this example to see how it works: 

```{r}
# 
heart$oldpeak[c(50, 100, 150, 200, 250)] = NA
colSums(is.na(heart))
```

There are now 5 missing values in the "oldpeak" feature. Now, median impute the missing values! We also want to create missingness indicators to inform us about the location of missing data. These are additional columns we will add to our data frame that represent the _locations_ within each feature that have missing values - 0 means data are present, 1 means there was a missing (and soon to be imputed) value.  

> Note: since our data are on different scales we will also want to center and scale these data using the `scale` function if you select "standard" (median) imputation as the method. If you select "knn" as the method, values will be automatically centered and scaled. 

```{r impute_missing_values}
result = ck37r::impute_missing_values(heart, 
                               verbose = TRUE,
                               type = "standard")

# Use the imputed dataframe.
heart = result$data

# View new columns. Note that the indicator feature "miss_oldpeak" has been added as the last column of our data frame. 
str(heart)

# No more missing data!
colSums(is.na(heart))
```

Scale the data to a mean of 0 and standard deviation of 1. Note that the `skip_vars` argument will exclude variables from the scaling process. 

Since the "ca", "cp", "slope", and "thal" features are currently interger type, first convert them to factors to then convert them to indicators. The other relevant variables are either continuous or are already indicators (just 1's and 0's). 

```{r}
# Make copy of the heart dataset so that the original is preserved
heart2 = heart

heart2$ca = as.factor(heart2$ca)
heart2$cp = as.factor(heart2$cp)
heart2$slope = as.factor(heart2$slope)
heart2$thal = as.factor(heart2$thal)

# Inspect the factor version
str(heart2)
```

## Defining *y* outcome vectors and *x* feature dataframes

First, make a copy of `heart2` named `proc_data`

```{r}
proc_data = heart2
```

Now we can standardize the data and expand the factors into indicators. 

Standardize the data: 

```{r}
proc_data = ck37r::standardize(heart2, 
                               skip_vars = c("age", "sex", "ca", "cp", 
                                             "slope", "thal", "target",
                                             result$indicators_added))

# Note that our factor (cp, slope, ca, thal) and age, sex, target, and miss_oldpeak features remain unscaled.
str(proc_data)
```                         
           
## Convert factors to indicators

Now expand "ca", "cp", "slope", and "thal" features to out into sparse indicators and overwrite `proc_data` with the updated version: 

```{r}
proc_data = as.data.frame(model.matrix(~ . ,  data = proc_data))

# Remove intercept
proc_data = proc_data[ , -1]

# Inspect
str(proc_data)
```

##### Regression setup

Now that the data have been imputed, standardized, and properly converted, we can assign the regression outcome variable (`age`) to its own vector for the OLS and lasso **REGRESSION tasks**. Remember that lasso can also perform classification as well. 

## Set seed for reperoducibility

Take the simple approach to data splitting and divide our data into training and test sets; 70% of the data will be assigned to the training set and the remaining 30% will be assigned to the holdout, or test, set.

```{r caret_split}
# Set seed for reproducibility.
set.seed(1)
```

# Random versus stratified random split

Since age is a continuous variable and will be the outcome for the OLS and lasso regressions, we will not perform a stratified random split like we will for the classification tasks (see below). Instead, [let's randomly assign](https://stackoverflow.com/questions/17200114/how-to-split-data-into-training-testing-sets-using-sample-function) 70% of the `age` values to the training set and the remaining 30% to the test set.

```{r}
# Define the sizes of training (70%) and test (30%) sets.
(training_size = floor(0.70 * nrow(proc_data)))

# Partition the rows to be included in the training set.
training_rows_reg = sample(nrow(proc_data), size = training_size)

# Partition the appropriate rows into the training and test sets
train_reg = proc_data[training_rows_reg, ] # partition training dataset
test_reg = proc_data[-training_rows_reg, ] # partition test dataset

# Remove y outcome from train_reg and test_reg and define it as its own variable for both: 
train_y_reg = train_reg$age
test_y_reg = test_reg$age

train_x_reg = subset(train_reg, select = -age)
test_x_reg = subset(test_reg, select = -age)
  
# Lngths of our y values and the number of rows in our feature dataframes are the same for both training and test sets!
dim(train_x_reg)
length(train_y_reg)

dim(test_x_reg)
length(test_y_reg)
```

##### Classification setup

Assign the outcome variable to its own vector for the decision tree, random forest, gradient boosted tree, and SuperLearner **CLASSIFICATION tasks**. However, keep in mind that these algorithms can also perform regression!  

Because we were savvy earlier, we can just pick up from where `proc_data` left off! 

```{r data_prep}
# Manually scale the "age" feature in place since we did not do this for regression setup since it was our outcome variable. 
proc_data$age = scale(proc_data$age, center = TRUE, scale = TRUE)

# View the updated data. age has been centered and scaled. 
str(proc_data)
```

This time however, "target" (1 = person has heart disease, 0 = person does not have heart disease) will by our y outcome variable - the others will by our x features. `y_class` will be our classification outcome ("target"), while `x_class` will contain the remaining classification features.   

```{r}
# Identify y and x variables. 
y_class = proc_data$target
x_class = subset(proc_data, select = -target)
```
           
Our factors have still been converted to indicators from the regression setup! :) 
# Stratified random split

We then can use [stratified random sampling](https://stats.stackexchange.com/questions/250273/benefits-of-stratified-vs-random-sampling-for-generating-training-data-in-classi) to divide our data into training and test sets; 70% of the data will be assigned to the training set and the remaining 30% will be assigned to the holdout, or test, set. 

```{r}
# Set seed for reproducibility
set.seed(1) 

# Create a stratified random split.
training_rows = caret::createDataPartition(y_class, p = 0.70, list = FALSE) 

train_x_class = x_class[training_rows, ] # partition training dataset
test_x_class = x_class[-training_rows, ] # partition test dataset

train_y_class = y_class[training_rows] # partition training y vector labels
test_y_class = y_class[-training_rows] # partition test y vector labels

# Check dimensions
dim(train_x_class)
length(train_y_class)

dim(test_x_class)
length(test_y_class)
```
