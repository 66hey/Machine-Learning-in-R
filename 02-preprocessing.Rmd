# Preprocessing

## Load the data
Load the "PimaIndiansDiabetes2" and "iris" datasets

```{r load_data}
library(mlbench)

# Load the PimaIndiansDiabetes2 dataset.
data("PimaIndiansDiabetes2") 

# Read background information and variable descriptions.
?PimaIndiansDiabetes2

# Rename the dataset to something simpler (pidd = "Pima Indians Diabetes Dataset").
pidd = PimaIndiansDiabetes2 

# View the structure of pidd.
str(pidd) 

# Also load iris dataset for challenge questions.
data(iris)
str(iris)

# Background info/variable descriptions.
?iris
```

## Data preprocessing

Data peprocessing is an integral first step in machine learning workflows. Because different algorithms sometimes require the moving parts to be coded in slightly different ways, always make sure you research the algorithm you want to implement so that you properly setup your Y and X variables and appropriately split your data into training and test sets if neeeded.  

One additional preprocessing aspect to consider: datasets that contain factor (categorical) features should typically be expanded out into numeric indicators (this is often referred to as [one-hot encoding](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f). You can do this manually with the `model.matrix` R function. This makes it easier to code a variety of algorithms to a dataset as many algorithms handle factors poorly (decision trees being the main exception). Doing this manually is always good practice. However, functions like `lm` will internally expand factor variables such as the `diabetes` factor predictor into numeric indicators.  

For the regression setup, we will use `model.matrix` to convert the "diabetes" status variable ("pos" or "neg") to an indicator, since Lasso expects a matrix of input variables. If we leave "diabetes" as a factor, it will convert the entire matrix to character type - an unacceptable input format for Lasso. 

> NOTE: Keep in mind that training/test dataset splitting is common, but not always preferred. We will introduce you to cross-validation in the second half of this workshop where _all_ the data are used and multiple training/testing splits are utilized. 

## Handling missing data

Missing values need to be handled somehow. Listwise deletion (deleting any row with at least one missing case) is common but this method throws out a lot of useful information. Many advocate for mean imputation, but arithmetic means are sensitive to outliers. Still, others advocate for Chained Equation/Bayesian/Expectation Maximization imputation (e.g., the [mice](http://www.stefvanbuuren.nl/publications/mice%20in%20r%20-%20draft.pdf) and [Amelia II](https://gking.harvard.edu/amelia) R packages).  

K-nearest neighbor imputation can also be useful, median imputation is demonstrated below:
```{r review_missingness}
# First, count the number of missing values across variables in our pidd dataset
colSums(is.na(pidd))

# Then, compute the proportion of missing values across all data cells in pidd
sum(is.na(pidd)) / (nrow(pidd) * ncol(pidd)) # ~9% of data points are missing
```

Now, median impute the missing values! We also want to create missingness indicators to inform us about the location of missing data. Thus, we will add some additional columns to our data frame.  

Neither the "diabetes" nor "age" columns have any missing values, so we can go ahead and impute the whole dataset.  

> Note: since our data are on different scales (e.g., age in years, number of times pregnant, etc.) we will also want to manually center and scale these data using the `scale` function if you select "standard" (median) imputation as the method. If you select "knn" as the method, values will be automatically centered and scaled. 

```{r impute_missing_values}
result = ck37r::impute_missing_values(pidd, 
                               verbose = TRUE,
                               type = "standard")

# Use the imputed dataframe.
pidd = result$data

# View new columns.
str(pidd)

# No more missing data and missingness indicators have been added as columns! 
colSums(is.na(pidd))
```

Scale the data to a mean of 0 and standard deviation of 1. Note that the `skip_vars` argument will exclude variables from the scaling process. 
```{r}
pidd = ck37r::standardize(pidd, skip_vars = c("age", "diabetes", result$indicators_added))

# Inspect
str(pidd)
```

## Defining Y outcome vectors and X feature dataframes

##### Regression setup

Assign the outcome variable (age) to its own vector for **REGRESSION tasks:** (OLS and lasso). Remember that lasso can also perform classification as well. Before splitting, convert the "diabetes" variable from factor to indicator type:

```{r data_prep}
# make a copy of pidd
pidd_reg = pidd

# View pidd variable names
names(pidd_reg)

# One hot encode diabetes from factor to indicator type
reg_feat = as.data.frame(model.matrix(~ . ,  data = pidd_reg))

head(reg_feat)

# Remove added Intercept column
reg_feat = reg_feat[, -1]

head(reg_feat)
```

We then can take the simple approach to data splitting and divide our data into training and test sets; 70% of the data will be assigned to the training set and the remaining 30% will be assigned to the holdout, or test, set. 
```{r caret_split}
# Set seed for reproducibility.
set.seed(1)

# Since age is a continuous variable and will be the outcome for lasso regression, we will not perform a stratified random split like we will for the classification tasks. Instead, let's randomly assign 70% of the data to the training set and the remaining 30% to the test set.

# Define the sizes of training (70%) and test (30%) sets.
(training_size = floor(0.70 * nrow(reg_feat)))

# Partition the rows to be included in the training set.
training_rows_reg = sample(seq_len(nrow(reg_feat)), size = training_size)

# Partition the appropriate rows into the training and test sets
train_x_reg = reg_feat[training_rows_reg, ] # partition training dataset
test_x_reg = reg_feat[-training_rows_reg, ] # partition test dataset

# Our outcome is still found in our features; define it as its own variable for training and test sets and remove it from our features
names(train_x_reg)
names(test_x_reg)

train_val_reg = train_x_reg$age # define the training set Y vector values (age)
test_val_reg = test_x_reg$age # define the test set Y vector values (age)

# Remove "age" from train_val_reg and test_val_reg
train_x_reg = subset(train_x_reg, select = -age)
test_x_reg = subset(test_x_reg, select = -age)

# "age" is no longer in our features
names(train_x_reg)
names(test_x_reg)

# lengths of our Y values and the number of rows in our training dataframes are the same for both training and test sets!
dim(train_x_reg)
length(train_val_reg)

dim(test_x_reg)
length(test_val_reg)
```

##### Classification setup

Assign the outcome variable to its own vector for **CLASSIFICATION tasks:** (decision tree, random forest, gradient boosting, and SuperLearner algorithms). However, keep in mind that these algorithms can also perform regression!

```{r data_prep}
# View pidd variable names
names(pidd)

# 1) Define Y for classification (has diabetes? "pos" or "neg")
y_fac = pidd$diabetes
head(y_fac, n = 20)

# 2) Then, convert "pos" to 1 and "neg" to 0. Many algorithms expect 1's for the positive class and 0's for the negative class.
y = as.integer(y_fac == "pos")
table(y, y_fac, useNA = "ifany")

# 3) Finally, define the X feature/predictor dataframe that excludes the Y outcome.
features = subset(pidd, select = -diabetes)

# Diabetes column has been successfully removed.
names(features)

# Finally, scale the "age" variable since we did not do this for purposes of regression setup
features$age = scale(features$age, center = T, scale = T)

# View the updated data
head(features)
```

We then can use [stratified random sampling](https://stats.stackexchange.com/questions/250273/benefits-of-stratified-vs-random-sampling-for-generating-training-data-in-classi) to divide our data into training and test sets; 70% of the data will be assigned to the training set and the remaining 30% will be assigned to the holdout, or test, set. 
```{r caret_split}
# Set seed for reproducibility.
set.seed(1)

# Create a stratified random split.
training_rows = caret::createDataPartition(y, p = 0.70, list = FALSE) 

train_x = features[training_rows, ] # partition training dataset
test_x = features[-training_rows, ] # partition test dataset

train_label = y[training_rows] # partition training Y vector labels
test_label = y[-training_rows] # partition test Y vector labels

# lengths of our Y label vectors and the number of rows in our training dataframes are the same for both training and test sets!
dim(train_x)
length(train_label)

dim(test_x)
length(test_label)
```
