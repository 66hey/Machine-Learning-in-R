---
title: "Introduction to Machine Learning in R"
author: "Evan Muzzall and Chris Kennedy"
date: "3/6/2017"
output:
  html_document: default
---

# 1. Pre-introduction
Be sure you have installed and libraried the following packages:
```{r}
if (FALSE) {
  # Run this line manually to install the necessary packages.
  install.packages(c("car", "caret","chemometrics", "class", "doParallel", "gbm", "ggplot2", "gmodels", "parallel", "pROC", "randomForest", "RhpcBLASctl", "rpart", "rpart.plot", "SuperLearner"))
}
  
library(car)
library(caret)
library(chemometrics)
library(class)
library(doParallel)
library(gbm)
library(ggplot2)
library(gmodels)
library(parallel)
library(pROC)
library(randomForest)
library(RhpcBLASctl)
library(rpart)
library(rpart.plot)
library(SuperLearner)
```

# 2. A brief history of machine learning

Machine learning evolved from scientific pursuits in computational/information theory, artificial intelligence, and pattern recognition. 

##### Check out a few pioneers of computation, artificial intelligence, behavior modeling, and proto-machine learning:

[Blaise Pascal's calculator](http://history-computer.com/MechanicalCalculators/Pioneers/Pascal.html).  
[Turing AM. 1950. Computing Machinery and Intelligence. _Mind_ 59:433-460](http://www.jstor.org/stable/pdf/2251299.pdf)  
[Saygin AP, Cicekli I, Akman V. 2003 Turing Test: 50 Years Later. The Turing Test, pp. 23-78. Springer Netherlands](http://cogprints.org/1925/2/tt50.ps)  

[Samuels, A. 1959. Some Studies in Machine Learning Using the Game of Checkers. _IBM Journal of Research and Development_](http://ucelinks.cdlib.org:8888/sfx_local?sid=google&auinit=AL&aulast=Samuel&atitle=Some+studies+in+machine+learning+using+the+game+of+checkers&id=doi:10.1147/rd.33.0210&title=IBM+Journal+of+Research+and+Development&volume=3&issue=3&date=1959&spage=210&issn=0018-8646)  

[Rosenblatt F. 1958. The perceptron: A probabilistic model for information storage and organization in the brain. _Psychological Review_ 65:386-408](http://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf)  

##### The importance of statistics

[Welling's commentary](https://www.ics.uci.edu/~welling/publications/papers/WhyMLneedsStatistics.pdf) offers a glimpse about the importance of statistics to machine learning. Also see [Tavish Srivastava's discussion](https://www.analyticsvidhya.com/blog/2015/07/difference-machine-learning-statistical-modeling/) for a discussion about the differences between statistics and machine learning.  

Thus, the modern strength of machine learning lies in its breadth of application. It is now used to provide ["actionable insight"](https://www.techopedia.com/definition/31721/actionable-insight) for research, problem solving, and decision making in science, technological industries, business,  language, the social and biological sciences, and humanities.  

View the [DeepMind blog](https://deepmind.com/blog/understanding-agent-cooperation/) to learn about complex social dilemmas and cooperative behavior.  

# 3. Supervised machine learning

Machine learning algorithm selection depends on the nature of the problem being investigated. Algorithms are generally divided into three types: supervised, unsupervised, and reinforcement - we will focus on supervised learning methods for this workshop. Classification and regression are the two main subtypes of supervised learning.  

Supervised machine learning algorithms learn a target function _f_ that best maps to a response/target/outcome/dependent variable "Y" based on independent/input/predictor variable(s) "X". The algorithm then tries to predict Y based on new data for X. However, these predictions are influenced by three types of error: bias, variance, and irreducible error. See [An Introduction to Statistical Learning, Chapters 1 and 2](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Sixth%20Printing.pdf) to learn more.  

##### 3. Machine learning algorithms - classification or regression?

In supervised machine learning, classification and regression are common distinctions. Classification is used when your target Y variable is discrete, such as a binary Y outcome variable. Examples include predicting if an email is spam or not, estimating probability of mortality in the emergency room, whether a tumor is cancerous or benign, or how likely someone is to vote in the next election. Classification models the probability that the outcome variable is 1 based on the covariates: $Pr(Y = 1 | X)$. It can be extended to multiple categories as well.

Regression is used when the target Y variable is continuous. Examples include predicting income, GDP, housing prices, distance, height, or weight. Regression models the conditional expectation (conditional mean) of the outcome variable given the covariates: $E(Y | X)$.

##### 3. Machine learning algorithms - research design

It should again be emphasized that a major strength of machine learning is its practical application for investigative research. Before performing machine learning in R, one must consider the nature of the problem being investigated. Some examples include:  

_Business:_ a real estate agent may want to [predict the price of houses](https://arxiv.org/pdf/1609.08399.pdf) in a neighborhood based on their different individual features, proximity to schools, and tax and crime rates of the area.  

_Biological sciences:_ a climate scientist might use past and present weather data to try and [predict future weather patterns](http://www.geosci-model-dev-discuss.net/gmd-2015-273/gmd-2015-273.pdf).  

_Social sciences:_ a researcher might choose to [predict voter turnout](http://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.28.2.51) based on the number of people currently registered to vote and preliminary polling numbers.  

_Publich health:_ machine learning applications are useful to [predict outbreaks of infectious disease](http://journals.plos.org/plosntds/article?id=10.1371/journal.pntd.0004549)  

_Humanities:_[Text analysis](http://www.digitalhumanities.org/dhq/vol/3/2/000041/000041.html) and social network analysis figure heavily into [humanities research](http://www.digitalhumanities.org/dhq/vol/3/2/000044/000044.html). 

##### 3. Machine learning algorithms - data preprocessing

Once an appropriate algorithm is selected, a common first step is to split a dataset into "training" and "test" datasets. A training dataset usually consists of a majority portion of the original dataset so that the algorithm can learn the model. The remaining portion of the dataset is then designated to the test dataset, which is used to evaluate the model fit to the new data.  

##### 3. Machine learning algorithms - performance metrics

The "test" dataset is a handy way to evaluate model performance. If an algorithm performs poorly on the training dataset, it could be said to be _underfit_ because it could not discern the relationships between the X and Y variables. A model is said to be _overfit_ if it performs well on the training dataset but poorly on the test dataset because it was not able to generalize to new data.  

Some common performance metrics for classification include accuracy, true positive/true negative rates, and area under the ROC curve; these are usually estimated using cross-validation. For regression we often use mean-squared error or mean-absolute error as performance metrics. See the below links for more information:  

[Jason Brownlee - How to Evaluate Machine Learning Algorithms](http://machinelearningmastery.com/how-to-evaluate-machine-learning-algorithms/) and [Assessing and Comparing Classifier Performance with ROC Curves](http://machinelearningmastery.com/assessing-comparing-classifier-performance-roc-curves-2/)  

[An Introduction to Statistical Learning - ROC, pp.147-149](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Sixth%20Printing.pdf)  

[R-bloggers - cross validation example](https://www.r-bloggers.com/cross-validation-for-predictive-analytics-using-r/)  

[Rob J. Hyndman - Why every statistician should know about cross-validation](http://robjhyndman.com/hyndsight/crossvalidation/)  

# 4. Load the Mroz dataset

Let's get started!  

We will use the "Mroz" dataset from the ["car" R package](https://cran.r-project.org/web/packages/car/index.html) for the examples in this workshop. The "iris" dataset will also be used for challenge questions.  

The Mroz dataset contains informaiton about women's participation in the workforce in 1975. Run `?Mroz` to read the variable descriptions. The original research article can be found [here](http://eml.berkeley.edu/~cle/e250a_f13/mroz-paper.pdf).  

The goal of this workshop is to help you fit various algorithms individually to learn about their construction, and then compare them simultaneously using the ["SuperLearner" R package](https://cran.r-project.org/web/packages/SuperLearner/index.html).  

We are interested a binary outcome variable - whether or not a woman participated in the workforce in 1975 based on the other variables as predictors. The exception for this workshop is for the regression example where we predict the expected wage rate instead.
```{r}
library(car)
data(Mroz)
?Mroz # read variable descriptions
str(Mroz) # 753 rows and 8 columns
```

# 5. k-Nearest Neighbor (kNN)

The k-Nearest Neighbor (kNN) algorithm is a good starting point for introducing machine learning because it makes no assumptions about the underlying distribution of the data. To classify a data point, kNN uses the characteristics of the points around it to determine how to classify it. For this example, point distances are measured via Euclidean distance.  

"k" is the number of neighbors used to classify the point. Choosing a proper "k" is integral to finding the best-performing model. *Large k-values* might be bad because then the largest class size would win regardless of the other surrounding points. *Small k-values* are bad because results will become sensitive to the influence of outliers while disregarding the proper influences of other neighboring points.  

##### 5. k-Nearest Neighbor (kNN) - data preprocessing

For k-nearest neighbors we need to convert categorical factors to numeric indicators, because kNN uses [euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance) to find the closest neighbors. We use `model.matrix()` to expand factors into a series of indicators, which is known as "one-hot encoding". `lm()` and `glm()` internally use model.matrix() to do this. Converting factors to indicators makes it easier to analyze a dataset with a variety of algorithms, because most algorithms can't handle factors (decision trees are the main exception).  
```{r}
str(Mroz)

# Assign outcome to its own vector.
Y <- Mroz$lfp

# Remove our outcome variable from the Mroz dataframe, so it is now only X variables. 
# Give it a new name ("data") 
data <- subset(Mroz, select = -lfp)

# Convert factors to indicators; this will add an intercept that we don't need. Let's also rename our dataframe "data":
data <- data.frame(model.matrix( ~ ., data)) 

# Notice the first column.
colnames(data)

# Remove the extra intercept column.
data <- data[, -1]

# Review data structure.
str(data)
```
Note that our Y response variable (Y) and our X predictor variables (data) are now contained within separate objects. Remember that the factors have been expanded out into indicators thanks to `model.matrix`.  

##### 5. k-Nearest Neighbor (kNN) - split the dataset

70% will be assigned to the training dataset, while the remaining 30% will be ascribed to the test dataset. 
```{r}
library(caret)
set.seed(1)
# Create a stratified random split
split <- createDataPartition(Y, p=0.70, list=FALSE) 

train <- data[split, ] # partition training dataset
test <- data[-split, ] # partition test dataset

train_label <- Y[split] # partition training Y variable vector
test_label <- Y[-split] #  partition test Y variable vector
```

##### 5. k-Nearest Neighbor (kNN) - selecting a starting "k-value"

Different methods exist for choosing a start point for "k". Let's use the square root of the number of rows in the training datset:

```{r}
round(sqrt(nrow(train)), digits=2) # 22.98
```

Fit the model! Our goal is to predict the classification accuracy of our Y variable (no/yes whether or not a woman participated in the workforce) based on the other seven X predictor variables:
```{r}
library(class)
set.seed(1)
data_pred <- knn(train = train, test = test,
                     cl = train_label, k = 23, prob = TRUE)
```

Using accuracy as our performance metric, display a contingency table to see how well the kNN algorithm predicted "No" and "Yes":
```{r}
library(gmodels)
CrossTable(x = test_label, y = data_pred, 
           prop.chisq = FALSE,
           prop.r = FALSE,
           prop.c = FALSE,
           prop.t = FALSE)

# Compute accuracy.
mean(test_label == data_pred)
```
How did it do?  

##### 5. k-Nearest Neighbor (kNN) - improving performance metrics

Two common ways of improving kNN performance are to 1) standardize the data so that scale does not unduly influence classification, and 2) change the k-value. 

##### 5. k-Nearest Neighbor (kNN) - improving performance metrics - 1) scale the data

Scaling the data is useful so that variables with large values do not bias the prediction. Create a new copy of the "data" dataset called "data_scaled", which will contain scaled values with means equal to 0 and standard deviations equal to 1. Our "Y" variable remains unchanged:
```{r}
data_scaled <- scale(data, center = TRUE, scale = TRUE)

# Look at the new distribution.
summary(data_scaled)
boxplot(data_scaled, las=2)
```

1) split the scaled data , 2) fit the model on the scaled data, and 3) examine the accuracy of predictions in the contingency table.

```{r}
# Use our existing split vector.

# Create the training partition.
train_scaled <- data_scaled[split, ] 

# Create the test partition.
test_scaled <- data_scaled[-split, ] 

# Extract outcome data for training set.
train_label_scaled <- Y[split] 

# Extract outcome data for test set.
test_label_scaled <- Y[-split] 
```

2) fit the model on scaled data
```{r}
library(class)
set.seed(1)
data_pred_scaled <- knn(train = train_scaled,
                            test = test_scaled, 
                            cl = train_label_scaled,
                            k = 23, prob = TRUE)
```

3) examine the accuracy of predictions in the contingency table
```{r}
library(gmodels)
CrossTable(x = test_label_scaled, y = data_pred_scaled, 
           prop.chisq = FALSE,
           prop.r = FALSE,
           prop.c = FALSE,
           prop.t = FALSE)

# Compute accuracy directly.
mean(test_label_scaled == data_pred_scaled)
```
What happened?  

##### 5. k-Nearest Neighbor (kNN) - improving performance metrics - 2) change the k-value

It also helps to investigate the cross-validated errors for multiple k-values at once to see which is ideal. Plot the cross-validated errors:
```{r}
library(chemometrics)
set.seed(1)
# png("kNN classification.png", height=6, width=9, units="in", res=600)
knn_k <- knnEval(data_scaled, Y, split, 
                 knnvec=seq(1, 50, by = 5), 
                 legpo="bottomright", las = 2)
# dev.off()
```

A combination of scaling the data and searching for the best cross-validated "k" is ideal.  

### Challenge 1  

1. What happens to accuracy when k = 1, 2, or 3?  
2. What is the optimal k-value from the cross-validation plot? What happens to the predictive accuracy when you switch k to this value?  
3. Using what you learned above, how would you classify the predictive accuracy of the Species vector in the  "iris" dataset? Hint: use `data(iris)` to load the dataset.

# 6. Linear regression
OLS regression can be used when the target Y variable is continuous.  

Remember that under the hood, `lm` and `glm` are one hot encoding factors to indicators. Write it out again for good practice: 
```{r}
# View structure of Mroz.
str(Mroz)

# Define our continuous Y response variable, "lwg". Call it "Y2".
Y2 <- Mroz$lwg

# Expand our predictor X variables out into indicators.
data2 <- data.frame(model.matrix( ~ ., subset(Mroz, select = -lwg)))

# Remove intercept.
data2 <- data2[, -1]

# View structure of "data2".
str(data2)
```

Fit a model that predicts the Y variable log wage gap ("lwg") via the other X predictor variables. Mean squared error (MSE) will be our performance metric. MSE measures the difference between observed and expected values, with smaller values tending to reflect greater predictive accuracy. 
```{r}
# Fit the regression model.
data2_lm <- lm(Y2 ~ ., data = data2)

# View the regression results.
summary(data2_lm) 

# Predict the outcome back onto the training data.
data2_pred <- predict(data2_lm, data2) 

# Calculate mean-squared error. 
MSE <- mean((Y2 - data2_pred)^2)   

MSE
```

See the [yhat blog for Fitting & Interpreting Linear Models in R](http://blog.yhat.com/posts/r-lm-summary.html) to learn more about `lm` output.  

### Challenge 2

1) What happens to MSE when you try to predict "age" and "lfp"?  
2) Set up a regression model that predicts one of the numeric variables from the "iris" dataset. 

# 7. Decision trees 

Decision trees are recursive partitioning methods that attempt to classify data by dividing it into subsets according to a Y output variable based on the other X predictor variables. Let's see how a tree-based method classifies women's participation in the workforce.  

Note that we do not have to use `model.matrix` for our decision tree algorithm here:
```{r}
library(rpart)

# Choose method="anova" for a regression tree; also see the "control" argument
# in the help files.
dt <- rpart(Y ~ ., data = subset(Mroz, select = -lfp), method = "class")

# Here is the text-based display of the decision tree.
print(dt)

# The plot is much easier to interpret.
library(rpart.plot)
rpart.plot(dt)
```

In decision trees the main hyperparameter (configuration setting) is the *complexity parameter* (CP). A high CP results in a simple decision tree with few splits, whereas a low CP results in a larger decision tree with many splits. So, the name is a little counterintuitive.  

rpart uses cross-validation internally to estimate the accuracy at various complexity parameter (CP) settings. We can review those to see what setting seems best.
```{r}
# Print the results for various CP settings.
# Here we want the setting with the lowest "xerror".
printcp(dt)

# Plot the performance estimates for different CP settings.
# We see that a larger tree (lower CP) seems more accurate.
plotcp(dt) 

# We can "prune" a tree to a higher CP setting if we want.
# Note: due to rounding we make the cp value slightly higher than
# was shown in the table.
tree_pruned <- prune(dt, cp = 0.033847)

# Print detailed results, variable importance, and summary of splits.
summary(tree_pruned) 
rpart.plot(tree_pruned)
```

Here, each node shows the cutoff for each of the predictors and how it contributed to the split. View a helpful decision tree tutorial at [GormAnalysis](https://gormanalysis.com/decision-trees-in-r-using-rpart/).  

### Challenge 3
1) Make a decision tree that tries to classify something other than "lfp".
2) What are the "minsplit" and "cp" hyperparameters within the "control" parameter? How might you go about figuring out what these are?  
HINT: the syntax might look like this: `dt2_control <- rpart.control(minsplit = 20, cp = 0.001)`  

# 8. Random forests
Random forests also are recursive partitioning methods that use multiple decision trees for ensemble predictions for classification and regression. At each random forest tree split, only a small portion of the predictors are used (rather than the full suite).  

The decision tree method did not require us to split the data into training and test sets. Thus, we can use our previously defined scheme of "Y" and "data" variables and "train" and "test" datasets: 
```{r}
# view "Y" and "data"
table(Y)
str(data)

# split this data, similar to before
str(train)
str(test)
```

Now, let's fit a model that tries to predict the number of women who participated in the labor force in 1975 again using the other variables as our X predictors: 
```{r}
library(randomForest)
set.seed(1)
rf1 <- randomForest(train_label ~ ., 
                    data = train, 
                    # Number of trees
                    ntree = 500, 
                    # Number of variables randomly sampled as candidates at each split.
                    mtry = 2, 
                    # We want the importance of predictors to be assessed.
                    importance = TRUE) 

rf1

```
The "OOB estimate of error rate" shows us how accurate our model is. $accuracy = 1 - error rate$.  

We can examine the relative variable importance in table and graph form. Random Forest estimates variable importance by separately examining each variable and estimating how much the model's accuracy drops when that variable's values are randomly shuffled. The shuffling temporarily removes any relationship between that covariate's value and the outcome. If a variable is important then the model's accuracy will suffer a large drop when it is randomly shuffled. But if the model's accuracy doesn't change it means the variable is not important to the model - e.g. maybe it was never chosen as a split in any of the decision trees.
```{r}
# As the function name suggests, this creates a variable importance plot.
varImpPlot(rf1)

# Raw data.
rf1$importance
```
You can read up on the [gini coefficient](https://en.wikipedia.org/wiki/Gini_coefficient) if interested. It's basically a measure of diversity or dispersion - a higher gini means the model is classifying better. The gini version does not randomly shuffle the variable cells.  

Now, the goal is to see how the model performs on the test dataset:
```{r}
rf_pred <- predict(rf1, newdata=test)
table(rf_pred, test_label)
```

Check the accuracy of the test set:
```{r}
mean(rf_pred == test_label) # 0.7377778

# Or, by hard-coding the table counts:
(66 + 100) / nrow(test) # 0.7377778
```
How did it do? Are the accuracies for the training and test sets similar?  

# Challenge 4

1. Select another Y variable to predict and evaluate performance on the train_rf and test_rf. What happens to the fit of the model when another variable is chosen?  

# 8. Boosting
"Boosting is a general method for improving the accuracy of any given learning algorithm" and evolved from AdaBoost and PAC learning (p. 1-2). Gradient boosted machines are ensembles decision tree methods of "weak" trees that are just slightly more accurate than random guessing. These are then "boosted" into "strong" learners. That is, the models don't have to be accurate over the entire feature space.  

The model first tries to predict each value in a dataset - the cases that can be predicted easily are _downweighted_ so that the algorithm does not try as hard to predict them.  

However, the cases that the model has difficulty predicting are _upweighted_ so that the model tries more assertively to predict them. This continues for multiple "boosting iterations", with a resample-based performance measure produced at each iteration. Error is measured on the weak learners so that even performing slightly better than random guessing improves accuracy fast (p.2). This method can drive down generalization error thus preventing overfitting (p. 5). While it is susceptible to noise, it is robust to outliers. (from [Freund Y, Schapire RE. 1999. A short introduction to boosting. Journal of Japanese Society for Artificial Intelligence 14:771-780](https://cseweb.ucsd.edu/~yfreund/papers/IntroToBoosting.pdf).  

Remember, our data are still split from before:
```{r}
# view "Y" and "data"
table(Y)
str(data)
nrow(train) + nrow(test) == nrow(data) # sanity check
```

Rather than testing only a single model at a time, it is useful to compare tuning parameters within that single model. Bootstrap is often a default, but we want cross-validation. We also want to specify different tuning parameters.  

Let us first create two objects - `control` and `grid`. `control` will allow us to tune the cross-validated performance metric, while `grid` lets us evaluate the model with different characteristics: 
```{r}
# Choose 10-fold repeated measure cross-validation as our performance metric
# (instead of the default "bootstrap").
control <- trainControl(method = "repeatedcv", 
	repeats = 2,
	# Calculate class probabilities.
	classProbs = TRUE,
	# Indicate that our response varaible is binary.
	summaryFunction = twoClassSummary) 

grid <- expand.grid(
  # Number of trees to fit, aka boosting iterations.
  n.trees = seq(500, 1500, by = 500),
  # Depth of the decision tree.
	interaction.depth = c(1, 2, 3), 
  # Learning rate: lower means the ensemble will adapt more slowly.
	shrinkage = c(0.01, 0.05, 0.1),
  # Stop splitting a tree if we only have this many obs in a tree node.
	n.minobsinnode = 10)
```

Fit the model. Note that we are now using the area under the ROC curve as our performance metric. This can be graphed for quick visualization: 
```{r}
library(caret)
library(pROC)
set.seed(1)
gbm1 <- train(train_label ~ ., data = cbind(train_label, train), # cbind: caret expects the Y response and X predictors to be part of the same dataframe
	method = "gbm",
	# Use AUC as our performance metric, which caret incorrectly calls "ROC".
	metric = "ROC",
	trControl = control,
	tuneGrid = grid,
	# Keep output more concise.
	verbose = FALSE)

# See how long this algorithm took to complete.
gbm1$times 

# Review model summary table.
gbm1 

# Plot variable importance.
summary(gbm1, las = 2)

# Generate predicted values.
gbm.pred <- predict(gbm1, test)

# Generate class probabilities.
gbm.prob <- predict(gbm1, test, type = "prob")

# Define ROC characteristics.
rocCurve <- roc(response = test_label,
	predictor = gbm.prob[, "yes"],
	levels = rev(levels(test_label)),
	auc=TRUE, ci=TRUE)

# Plot ROC.
plot(rocCurve, print.thres = "best", main = "GBM", col = "blue") 
# ggsave("gbm AUC.png")

# Plot the cross-validated AUC of the different configurations.
ggplot(gbm1) + theme_bw() + ggtitle("GBM model comparisons") 
# ggsave("gbm tuning comparison.png")
```

Also check out the ["xgboost" R package](https://cran.r-project.org/web/packages/xgboost/index.html) for extreme gradient boosting.  

### Challenge 5

1. Using your Mroz dataset, fit a GBM model where "wc" is your Y variable. What happens to variable relative importance? What happens to AUC?

# 9. Ensemble methods

The ["SuperLearner" R package](https://cran.r-project.org/web/packages/SuperLearner/index.html) is a method that simplifies ensemble learning by allowing you to simultaneously evaluate the cross-validated performance of multiple algorithms and/or a single algorithm with differently tuned hyperparameters.  

Let's see how the five algorithms you learned in this workshop (kNN, linear regression, decision trees, random forest, and gradient boosted machines) compare to each other and to the mean of Y as a benchmark algorithm, in terms of their cross-validated error!

A "wrapper" is a short function that adapts an algorithm for the SuperLearner package. Check out the different algorithm wrappers offered by SuperLearner:
```{r}
library(SuperLearner)
listWrappers()
```

Instead of splitting the data like before, since we are using cross-validation we actually want to use the entire `Mroz` dataset:
```{r}
str(Mroz)

# convert our Y variable to numeric type
Mroz$lfp <- ifelse(Mroz$lfp == "yes", 1, 0)

# Now, tell SuperLearner which algorithms to urn. In the interest of time we are not including GBM, but "SL.gbm" is the wrapper.
SL_library <- c("SL.mean", "SL.knn", "SL.glm", "SL.rpart", "SL.randomForest")

```

Fit the ensemble:

```{r}
library(SuperLearner)
# This is a seed that is compatible with multicore parallel processing.
# See ?set.seed for more information.
set.seed(1, "L'Ecuyer-CMRG") 

# This may take a minute to execute.
cv_sl <- CV.SuperLearner(Y = Mroz$lfp, X = data,
                         SL.library = SL_library,
                         family = binomial(),
                         # Fit only 5 folds for demo purposes only.
                         # For a real analysis we would do 20 folds.
                         cvControl = list(V = 5),
                         # Set to T to show details.
                         verbose = F)
```

Risk is a performance estimate - it's the average loss, and loss is how far off the prediction was for an individual observation. The lower the risk, the fewer errors the model makes in its prediction. SuperLearner's default loss metric is squared error $(y_{actual} - y_{predicted})^2$, so the risk is the mean-squared error (just like in ordinary least _squares_ regression). View and plot results:
```{r}
summary(cv_sl)

plot(cv_sl) + theme_bw()

# ggsave("SuperLearner.pdf")
```
"Discrete SL" is when the SuperLearner chooses the single algorithm with the lowest risk. "SuperLearner" is a weighted average of multiple algorithms, or an "ensemble". In theory the weighted-average should have a little better performance, although they often tie. In this case we only have a few algorithms so the difference is minor.

### Challenge 6. 
1. If you set the seed again and re-run `CV.SuperLearner` do you get exactly the same results?
2. Choose your favorite 2 algorithms and re-run the SuperLearner with just those algorithms in the library. Does the performance change?
3. What are the elements of the CV_SL object? Take a look at 1 or 2 of them. Hint: use the `names()` function to list the elements of an object, then `$` to access them (just like a dataframe).

A longer tutorial on SuperLearner is available here: https://github.com/ck37/superlearner-guide