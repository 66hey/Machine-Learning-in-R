---
title: "Introduction to Machine Learning in R"
author: "Evan Muzzall and Chris Kennedy"
date: "8/27/2018"
output:
  html_document:
    toc: yes
    toc_float: yes
---

# Introduction 

Visit the UC Berkeley [D-Lab](http://dlab.berkeley.edu/) to learn more about our services and resources, including the [Machine Learning Working Group](http://dlab.berkeley.edu/working-groups/machine-learning-working-group-0).  

## Resources

_An Introduction to Statistical Learning - with Applications in R (2013)_ by Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani. Springer: New York. [Amazon](https://www.amazon.com/Introduction-Statistical-Learning-Applications-Statistics/dp/1461471370) or [free PDF](http://www-bcf.usc.edu/~gareth/ISL/). We encourage you to support the authors by purchasing their textbook!

## Package installation

The following packages are required to run the code in this workshop:

```{r setup, eval = TRUE}
if (FALSE) {
  # Run these lines manually (once) to install the necessary packages.
  
  # Install packages from CRAN.
  install.packages(c(# Algorithms
                     "class", "gbm", "randomForest", "ranger", "rpart", "xgboost",
                     # Visualization
                     "ggplot2", "rpart.plot", 
                     # Machine learning frameworks
                     "caret", "SuperLearner",
                     # R utility packages
                     "devtools", "dplyr",
                     # Misc
                     "gmodels", "mlbench", "pROC", "chemometrics"))
  
  # Install packages not on CRAN or with old version on CRAN.
  devtools::install_github(c("ck37/ck37r"))
}

# Hide the many messages and possible warnings from loading all these packages.
suppressMessages(suppressWarnings({  
  library(caret)        # createDataPartition creates a stratified random split
  library(chemometrics) # knnEval algorithm
  library(ck37r)        # impute_missing_values, SuperLearner helpers
  library(class)        # K nearest neighbors
  library(devtools)     # Allows installing packages from github
  library(dplyr)        # Data cleaning
  library(gbm)          # Gradient boosted machines
  library(ggplot2)      # Graphics 
  library(gmodels)      # Nice cross tabulations
  library(magrittr)     # Pipes %>% for data cleaning, installed by dplyr.
  library(pROC)         # Compute and plot AUC 
  library(randomForest) # random forest algorithm
  library(rpart)        # Decision tree algorithm
  library(rpart.plot)   # Decision tree plotting
  library(SuperLearner) # Ensemble methods
}))
```

## Brief history of machine learning

Machine learning evolved from scientific pursuits in statistics, computer science, information theory, artificial intelligence, and pattern recognition.  

How to define machine learning?  
1) **In general:** algorithms, computers, and other machines that can "learn" without direct input from a human programmer.  
2) **Practically:** sets of tools for investigating/modeling/understanding data.  
3)  **Specifically:** (see below)

A proto-example:  
- [Pascal's calculator](http://history-computer.com/MechanicalCalculators/Pioneers/Pascal.html)  

Rapid advances:   
- [McCulloch Pitts neuron model](http://wwwold.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node12.html)  
- [Turing test](http://www.jstor.org/stable/pdf/2251299.pdf)  
- [Rosenblatt's perceptron](http://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf)  
- [Samuels and the game of checkers](http://ucelinks.cdlib.org:8888/sfx_local?sid=google&auinit=AL&aulast=Samuel&atitle=Some+studies+in+machine+learning+using+the+game+of+checkers&id=doi:10.1147/rd.33.0210&title=IBM+Journal+of+Research+and+Development&volume=3&issue=3&date=1959&spage=210&issn=0018-8646)  

Modern topics:  
- [Turing Test: 50 years later](http://www.cs.bilkent.edu.tr/~akman/jour-papers/mam/mam2000.pdf)  
- [computer vision](http://www.sciencedirect.com/science/article/pii/S1071581916301264)  
- [data cleaning](http://www.betterevaluation.org/sites/default/files/data_cleaning.pdf)  
- [robotics](https://arxiv.org/abs/1708.04677)  
- [cloud computing](https://arxiv.org/abs/1707.07452)  

The importance of statistics:  
- [Welling's commentary](https://www.ics.uci.edu/~welling/publications/papers/WhyMLneedsStatistics.pdf)  
- [Srivastava's discussion](https://www.analyticsvidhya.com/blog/2015/07/difference-machine-learning-statistical-modeling/)  
- [Breiman's take](https://projecteuclid.org/euclid.ss/1009213726)  

Seek "actionable insight":  
- ["actionable insight"](https://www.techopedia.com/definition/31721/actionable-insight)  

# Supervised machine learning

Selecting a machine learning algorithm depends on the characteristics of the problem being investigated - there is no "best" method applicable to all cases. Machine learning is generally divided into three broad classes of learning: [supervised](https://en.wikipedia.org/wiki/Supervised_learning), [unsupervised](https://en.wikipedia.org/wiki/Unsupervised_learning), and [reinforcement](https://en.wikipedia.org/wiki/Reinforcement_learning). In this workshop we will focus on two main subtypes of supervised machine learning: classification and regression.  

The syntax for supervised machine learning algorithms can be thought of like this:  

Y ~ X~1~ + X~2~ + X~3~â€¦ X~n~

Y is the dependent/response/target/outcome variable  
X are the independent/input/predictor/feature variables  

Supervised machine learning methods learn a target function $f$ that best maps X to Y based on a set of [training data](https://en.wikipedia.org/wiki/Training,_test,_and_validation_sets). 

Our function would look like this: $y = f(X) + \epsilon$, where $f$ is some function that relates our X predictor variables to Y in an unknown way thus we must estimate it. Epsilon $\epsilon$ is the random error, is independent of X, and averages to zero. Therefore, we can predict Y using $\hat{y} = \hat{f}(X)$ for new data (call the test dataset) and evaluate how well the algorithm learned the target function when introduced to new data.  

**How to define machine learning? (revisited)**  
More specifically, we can think of machine learning as a bunch of methods to estimate $f$!  

##### Classification or regression?

**Classification** is used when the Y outcome variable is categorical/discrete. Binary examples generally refer to a yes/no situation where a 1 is prediction of the "yes" category and 0 as the "no". Classification models the probability that the outcome variable is 1 based on the covariates: $Pr(Y = 1 | X)$. This can be extended to multi-level classification as well.  

**Regression** is used when the target Y outcome variable is continuous. Regression models the conditional expectation (conditional mean) of the outcome variable given the covariates: $E(Y | X)$.  

##### Data preprocessing

A longstanding first step is to split a dataset into **"training"** and **"test"** subsets. A training sample usually consists of a majority of the original dataset so that an algorithm can learn the model. The remaining portion of the dataset is designated to the test sample to evaluate model performance on data the model has not yet seen. **Missing data should be handled** before the splitting process commences.  

##### Model performance

**Performance metrics** are used to see how well a model predicts a specified outcome on training and test datasets.  

A model that performs poorly on the training dataset is **underfit** because it is not able to discern relationships between the X and Y variables.  

A model that performs well on the training dataset but poorly on the test dataset is said to be **overfit** because the model performed worse than expected when given new data. To some extent the patterns found in the training data may have been random noise and therefore, by random chance, are different in the test data.  

##### Common performance metrics

- Accuracy  
- Mean squared error  
- Sensitivity and specificity  
- Area under the ROC curve (AUC)  

# Workshop goals

##### General goals

1) Learn the basics of using six machine learning algorithms in R:  
  - k-nearest neighbor  
  - linear regression  
  - decision tree  
  - random forest  
  - boosting  
  - SuperLearner  
2) Examine the performance of these models  
3) Vizualize important information:  
  - knn accuracy tables  
  - decision trees  
  - random forest variable importance  
  - AUC from different boosting models  
  - SuperLearner cross-validated risk  
4) Simultaneously compare multiple algorithms in an ensemble  

##### Specific goals
Use the  `PimaIndiansDiabetes2` dataset from the [`mlbench` package](https://cran.r-project.org/web/packages/mlbench/mlbench.pdf) to investigate the following questions:  

1) **Binary classification examples:** How reliably can different machine learning algorithms predict a person's diabetes status using the other variables?  
2) **Regression example:** How well can a person's age be predicted using the other variables?  

What are these other variables? Let's load the data and find out!  

# Load the data
Load the `PimaIndiansDiabetes2` and `iris` datasets
```{r load_data}
library(mlbench)

# Load the PimaIndiansDiabetes2 dataset.
data("PimaIndiansDiabetes2") 

# Read background information and variable descriptions.
?PimaIndiansDiabetes2

# Rename the dataset to something simpler (pidd = "Pima Indians Diabetes Dataset").
pidd = PimaIndiansDiabetes2 

# View the structure of pidd.
str(pidd) 

# Also load iris dataset for challenge questions.
data(iris)
str(iris)

# Background info/variable descriptions.
?iris
```

# Data preprocessing

Data peprocessing is an integral first step in machine learning workflows. Because different algorithms sometimes require the moving parts to be coded in slightly different ways, always make sure you research the algorithm you want to implement so that you properly setup your Y and X variables and appropriately split your data into training and test sets if neeeded.  

One additional preprocessing aspect to consider: datasets that contain factor (categorical) features should typically be expanded out into numeric indicators (this is often referred to as [one-hot encoding](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f). You can do this manually with the `model.matrix` R function. This makes it easier to code a variety of algorithms to a dataset as many algorithms handle factors poorly (decision trees being the main exception). When we predict age in our regression example below, we will do this manually for the "diabetes" column to practice. However, functions like `lm` will internally expand factor variables such as the `diabetes` factor predictor into numeric indicators.  

> NOTE: Keep in mind that training/test dataset splitting is common, but not always preferred. We will introduce you to cross-validation in the second half of this workshop where _all_ the data are used and multiple training/testing splits are utilized. 

# Handling missing data

Missing values need to be handled somehow. Listwise deletion (deleting any row with at least one missing case) is common but this method throws out a lot of useful information. Many advocate for mean imputation, but arithmetic means are sensitive to outliers. Still, others advocate for Chained Equation/Bayesian/Expectation Maximization imputation (e.g., the [mice](http://www.stefvanbuuren.nl/publications/mice%20in%20r%20-%20draft.pdf) and [Amelia II](https://gking.harvard.edu/amelia) R packages).  

Median imputation is also useful, but knn imputation is demonstrated below for the classification examples
```{r review_missingness}
# First, count the number of missing values across variables in our pidd dataset
colSums(is.na(pidd))

# Then, compute the proportion of missing values across all data cells in pidd
sum(is.na(pidd)) / (nrow(pidd) * ncol(pidd)) # ~9% of data points are missing
```

Now, median impute the missing values! We also want to create missingness indicators to inform us about the location of missing data. Thus, we will add some additional columns to our data frame.  

Neither the "diabetes" nor "age" columns have any missing values, so we can go ahead and impute the whole dataset! 
```{r impute_missing_values}
library(ck37r)

result = impute_missing_values(pidd, verbose = TRUE)

# Use the imputed dataframe.
pidd = result$data

# view new columns
str(pidd)

# No more missing data and missingness indicators have been added as columns! 
colSums(is.na(pidd))
```

# Defining Y outcome vectors and X feature dataframes

##### Classification setup

Assign the outcome variable to its own vector for **CLASSIFICATION tasks:** in this workshop, k-nearest neighbor, decision tree, random forest, gradient boosting, and SuperLearner algorithms. However, keep in mind that these algorithms can also perform regression!

```{r data_prep}
# View pidd variable names
names(pidd)

# 1) Define Y for classification (has diabetes? "pos" or "neg")
y_fac = pidd$diabetes
head(y_fac, n = 20)

# 2) Then, convert "pos" to 1 and "neg" to 0. Many algorithms expect 1's for the positive class and 0's for the negative class.
y = as.integer(y_fac == "pos")
table(y, y_fac, useNA = "ifany")

# 3) Finally, define the X feature/predictor dataframe that excludes the Y outcome.
features = subset(pidd, select = -diabetes)

# Diabetes column has been successfully removed.
head(names(features))
```

We then can take the simple approach to data splitting and divide our data into training and test sets; 70% of the data will be assigned to the training set and the remaining 30% will be assigned to the holdout, or test, set. 
```{r caret_split}
library(caret)

# Set seed for reproducibility.
set.seed(1)

# Create a stratified random split.
training_rows = createDataPartition(y, p = 0.70, list = FALSE) 

train_x = features[training_rows, ] # partition training dataset
test_x = features[-training_rows, ] # partition test dataset

train_label = y[training_rows] # partition training Y vector labels
test_label = y[-training_rows] # partition test Y vector labels

# lengths of our Y label vectors and the number of rows in our training dataframes are the same for both training and test sets!
dim(train_x)
length(train_label)

dim(test_x)
length(test_label)
```

##### Regression setup

We could do something similar for our lone **REGRESSION task:** linear regression. 

We will use R's handy `predict` function to simply create a pseudo-test sample using predictions on all the data. 

# Algorithm walkthroughs
# 1. k-nearest neighbor (knn)
The k-nearest neighbor (knn) algorithm is a good machine learning start point because it makes no assumptions about the underlying distribution of the data. To classify a data point, knn uses the characteristics of the points around it to determine how to classify it. [Euclidean distances](https://en.wikipedia.org/wiki/Euclidean_distance) between points are used in this example.  

"k" is the number of neighbors used to classify the point in question. Choosing a proper "k" is integral to finding the best-performing model. **Large k-values** could be bad because the class with the largest size might win regardless of the influence of closer points. **Small k-values** might also be bad because neighboring points might become overly influential.  

##### 1.1 knn - selecting a starting k value
Different methods exist for choosing a start point for "k". Let's use the square root of the number of rows in the training datset: 
```{r}
(sqrt_rows = round(sqrt(nrow(train_x)))) # 23
```

Fit the model! Our goal is to predict the classification accuracy of our Y variable (whether or not a person has diabetes based on the other `train_x` predictor variables; 1 = yes/pos, 0 = no/neg:
```{r}
library(class)
# Set our seed because ties are randomly broken in knn.
set.seed(1)
knn_result = knn(train = train_x, test = test_x,
                 cl = train_label, k = sqrt_rows, prob = TRUE)
```

Using accuracy as our performance metric, compute a contingency table to see how well the model predicted yes and no:
```{r}
library(gmodels)
CrossTable(x = test_label, y = knn_result, chisq = FALSE,
           prop.chisq = FALSE, prop.r = FALSE, prop.c = FALSE, prop.t = FALSE)

# or
caret::confusionMatrix(knn_result, factor(test_label))

# Compute accuracy - how did we do?
mean(test_label == knn_result) # ~ 0.72
```
How did it do?  

##### 1.2 knn - improving performance metrics

Two common ways of improving model performance are to 1) standardize the data so that scale does not unduly influence classification, and 2) change the k-value. 

##### 1.3 knn - scale the data

Scaling the data is useful so that variables with large values do not bias the prediction. Create a new copy of the `features` dataset called `data_scaled`, which will contain scaled values with means equal to 0 and standard deviations equal to 1. Our Y variable remains unchanged:
```{r boxplots}
# Scale the dataset after first log-transforming insulin.
data_scaled = features %>% mutate(insulin = log(insulin)) %>%
  scale(center = TRUE, scale = TRUE) %>% data.frame()

# Quick function to make a boxplot for each variable in a dataset.
ggbox = function(df) {
  ggplot(stack(df),
        aes(x = factor(ind, levels = rev(names(df))), y = values)) +
  geom_boxplot() + theme_minimal() + coord_flip() + xlab("")
}

# Note the much larger scale for insulin, and to a lesser extent glucose.
ggbox(features) + ggtitle("Distribution of original features")
ggbox(data_scaled) + ggtitle("Distribution of rescaled features")
```

Repeat the process:  
1) split the scaled data  
```{r}
# Create the training set.
train_scaled = data_scaled[training_rows, ] 

# Create the test set.
test_scaled = data_scaled[-training_rows, ] 

# Extract outcome data for training set.
train_label_scaled = y[training_rows] 

# Extract outcome data for test set.
test_label_scaled = y[-training_rows] 
```

2) fit the model this time on the scaled data
```{r}
library(class)
set.seed(1)
knn_result_scaled = knn(train = train_scaled, test = test_scaled, 
                        cl = train_label_scaled, k = sqrt_rows, prob = TRUE)
```

3) examine the accuracy of predictions on the scaled data: 
```{r}
library(gmodels)
CrossTable(x = test_label_scaled, y = knn_result_scaled, 
           prop.chisq = FALSE, prop.r = FALSE, prop.c = FALSE, prop.t = FALSE)

# or
caret::confusionMatrix(knn_result_scaled, factor(test_label_scaled))

# Compute accuracy
mean(test_label_scaled == knn_result_scaled) # ~0.71
```

Did scaling the data help?  

##### 1.4 knn - improving performance metrics - change k-value

Scaling our data actually _reduced_ our accuracy a little, although we have not created confidence intervals for a formal statistical test of equality. It also helps to investigate the cross-validated accuracy for multiple k-values at once to see which is ideal. Plot the cross-validated error rates:

```{r cross-validated k}
library(chemometrics)

set.seed(1)
knn_k = knnEval(X = data_scaled, grp = y_fac, train = training_rows, 
                knnvec = c(1, 2, 3, 4, 5, 10, 20, 30, 40, 50),
                kfold = 20, las = 2)
```

We could choose the k that yields the lowest cross-validated error rate on the training data, or the k with the lowest test error rate. Looks like a k of 40 would be a good choice, especially compared to a small number like 1-4.

A combination of scaling the data and searching for the best cross-validated "k" can help maximize the accuracy of k-nearest neighbors. We don't have time to cover today but integrating kernels, alternative distance metrics, and feature selection are additional ways to maximize the performance.  

See [page 105 of Wu et al's 2018 paper](https://www.sciencedirect.com/science/article/pii/S2352914817301405) to see how our KNN stacks up to their comparative KNN reference study.  

**Big question 1:** What might you conclude about the k-nearest neighbors algorithm and this particular dataset? (see the solutions file for an answer)  

##### Challenge 1  
Using what you learned above, classify knn predictive accuracies of the `Species` variable in the "iris" dataset. 

# 2.  Linear regression
Ordinary least squares (OLS) regression can be used when the target Y variable is continuous. Remember that under the hood, `lm` is one-hot encoding factors to indicators, but we will write it out below just for good practice.  

This time, we will use the entire `pidd` dataset since we can use the `predict` R function to generate pseudo-test data. Here, we can just define our Y outcome inside the function (`pidd$age`) and then subset the rest of `pidd` to exclude age as a way to define our predictors.  

Mean squared error (MSE) and root mean squared error (RMSE) will be our performance metrics. MSE measures the difference between observed and expected values, with smaller values indicative of greater predictive accuracy. The advantage of RMSE is that it can be easier to interpret and explain because it is on the same unit scale as the outcome variable we are predicting. 
```{r}
# Fit the regression model.
reg = lm(age ~ ., data = pidd)

# View the regression results.
summary(reg) 

# Generate predicted values using the original (training) data.
predictions = predict(reg, pidd) 

# Calculate mean-squared error. 
(mse = mean((pidd$age - predictions)^2))

# Root mean squared error (RMSE).
sqrt(mse)
```

**Big question 2:** What might your surmise about linear regression and this dataset? (see the solutions file for an answer)  

##### Challenge 2
Code a regression model that predicts one of the numeric variables from the "iris" dataset. 

# 3.  Decision trees 
Decision trees are recursive partitioning methods that divide the predictor spaces into simpler regions and can be visualized in a tree-like structure. Decision trees attempt to classify data by dividing it into subsets according to a Y output variable and based on some predictors.  

Let's see how a decision tree classifies diabetes status from our dataset.  

Note that we do not have to use `model.matrix` for our decision tree algorithm here because it can work with factor variables directly:
```{r}
library(rpart)

tree = rpart(y ~ ., data = features,
             # Use method = "anova" for a continuous outcome.
             method = "class",
             # Can use "gini" for gini coefficient.
             parms = list(split = "information")) 

# Here is the text-based display of the decision tree. Yikes!  :^( 
print(tree)
```

Although interpreting the text can be intimidating, a decision tree's main strength is its tree-like plot, which is much easier to interpret.
```{r plot_tree}
library(rpart.plot)
rpart.plot(tree) # shadow.col = "lightgray", cex = 1, type = 4, extra = 101) 
```

We can also look inside of `tree` to see what we can unpack. "variable.importance" is one we should check out! 
```{r}
names(tree)
tree$variable.importance
```

In decision trees the main hyperparameter (configuration setting) is the **complexity parameter** (CP), but the name is a little counterintuitive; a high CP results in a simple decision tree with few splits, whereas a low CP results in a larger decision tree with many splits.  

`rpart` uses cross-validation internally to estimate the accuracy at various CP settings. We can review those to see what setting seems best.  

Print the results for various CP settings - we want the one with the lowest "xerror". We can also plot the performance estimates for different CP settings. 
```{r plotcp_tree}
# Show estimated error rate at different complexity parameter settings.
printcp(tree)

# Plot those estimated error rates.
plotcp(tree)

# 2 or 14 splits appear to be tied for lowest "xerror", but a tree with fewer splits might be easier to interpret. However, a tree with 14 splits has a lower relative error. 
tree_pruned2 = prune(tree, cp = 0.017724) # 2 splits

tree_pruned14 = prune(tree, cp = 0.010000) # 14 splits

# Print detailed results, variable importance, and summary of splits.
```
```{r eval = F}
summary(tree_pruned2) 
```
```{r plot_tree_pruned2}
rpart.plot(tree_pruned2)
```
```{r eval = F}
summary(tree_pruned14) 
```
```{r plot_tree_pruned14}
rpart.plot(tree_pruned14)
```

You can also get more fine-grained control by checking out the "control" argument inside the rpart function. Type `?rpart` to learn more.  

**Big question 3:** What do you notice about the tree with 2 splits and the tree with 14 splits? Are there any parts that are identical?  

##### Challenge 3
What are the "minsplit", "cp", and "minbucket" hyperparameters within the "control" parameter? Use the iris dataset to construct a decision tree that utilized the `rpart.control` hyperparameter.  

> HINT: the syntax might look like this: `ctrl = rpart.control(minsplit = 20, minbucket = 5, cp = 0.001)`  

# 4. Random forests
The random forest algorithm seeks to improve on the performance of a single decision tree by taking the average of many trees. Thus, a random forest can be viewed as an **ensemble** method, or model averaging approach. The algorithm was invented by UC Berkeley's own Leo Breiman in 2001, who was also a co-creator of decision trees (see his [1984 CART book](https://www.amazon.com/Classification-Regression-Wadsworth-Statistics-Probability/dp/0412048418)).  

Random forests are an extension of **bagging**, in which multiple samples of the original data are drawn with replacement (aka "bootstrap samples"). An algorithm is fit separately to each sample, then the average of those estimates is used for prediction. While bagging can be used by any algorithm, random forest uses decision trees as its base learner. Random forests add another level of randomness by also randomly sampling the features (or covariates) at each split in each decision tree. This makes the decision trees use different covariates and therefore be more unique. As a result, the average of these trees tends to be more accurate overall.

Fit a random forest model that predicts the number of people with diabetes using the other variables as our X predictors. If our Y variable is a factor, randomForest will by default perform classification; if it is numeric/integer regression will be performed and if it is omitted it will become unsupervised! 
```{r rf_fit}
library(randomForest)
set.seed(1)
(rf1 = randomForest(as.factor(train_label) ~ ., 
                   data = train_x, 
                   # Number of trees
                   ntree = 500, 
                   # Number of variables randomly sampled as candidates at each split.
                   mtry = 2, 
                   # We want the importance of predictors to be assessed.
                   importance = TRUE))
```

The "OOB estimate of error rate" shows us how accurate our model is. $accuracy = 1 - error rate$. OOB stands for "out of bag" - and bag is short for "bootstrap aggregation". So OOB estimates performance by comparing the predicted outcome value to the actual value across all trees using only the observations that were not part of the training data for that tree.

We can examine the relative variable importance in table and graph form. Random Forest estimates variable importance by separately examining each variable and estimating how much the model's accuracy drops when that variable's values are randomly shuffled (permuted). The shuffling temporarily removes any relationship between that covariate's value and the outcome. If a variable is important then the model's accuracy will suffer a large drop when it is randomly shuffled. But if the model's accuracy doesn't change it means the variable is not important to the model - e.g. maybe it was never even chosen as a split in any of the decision trees.
```{r rf_varImpPlot}
varImpPlot(rf1)

# Raw data
rf1$importance
```

You can read up on the [gini coefficient](https://en.wikipedia.org/wiki/Gini_coefficient) if interested. It's basically a measure of diversity or dispersion - a higher gini means the model is classifying better. The gini version does not randomly shuffle the variable values.  

Now, the goal is to see how the model performs on the test dataset:
```{r}
# This will predict the outcome class.
predicted_label = predict(rf1, newdata = test_x)
table(predicted_label, test_label)
```

Check the accuracy of the test set:
```{r prob_hist}
mean(predicted_label == test_label) # ~0.72

# We can also generated probability predictions, which are more granular.
predicted_prob = as.data.frame(predict(rf1, newdata = test_x, type = "prob"))
colnames(predicted_prob) = c("no", "yes")
summary(predicted_prob)
ggplot(predicted_prob, aes(x = yes)) + geom_histogram() + theme_minimal()

# devtools::install_github("ck37/ck37r")

# Review number of terminal nodes (aka "leaves") across the decision trees.
summary(ck37r::rf_count_terminal_nodes(rf1))
```

How did it do? Are the accuracies for the training and test sets similar?  

**Big question 4:** Why is the random forest algorithm preferred to a single decision tree or bagged trees?

##### Challenge 4

1. Try a few other values of mtry - can you find one that has improved performance?
2. Maxnodes is another tuning parameter for randomForest - does changing it improve your performance?
3. Use the iris dataset to perform classification on the "Species" variable. What are you noticing about model fits between the pidd and iris datasets? 

# 5. Boosting
from [Freund Y, Schapire RE. 1999. A short introduction to boosting. Journal of Japanese Society for Artificial Intelligence 14:771-780](https://cseweb.ucsd.edu/~yfreund/papers/IntroToBoosting.pdf):  

"Boosting is a general method for improving the accuracy of any given learning algorithm" and evolved from AdaBoost and PAC learning (p. 1-2). Gradient boosted machines are ensembles decision tree methods of "weak" trees that are just slightly more accurate than random guessing. These are then "boosted" into "strong" learners. That is, the models don't have to be accurate over the entire feature space.  

The model first tries to predict each value in a dataset - the cases that can be predicted easily are _downweighted_ so that the algorithm does not try as hard to predict them.  

However, the cases that the model has difficulty predicting are _upweighted_ so that the model more assertively tries to predict them. This continues for multiple "boosting iterations", with a training-based performance measure produced at each iteration. This method can drive down generalization error (p. 5). 

Rather than testing only a single model at a time, it is useful to tune the parameters of that single model against multiple versions. Also, bootstrap is the default, but we want cross-validation.  

First create two objects - `gbm_control` and `gbm_grid`. `gbm_control` will allow us to tune the cross-validated performance metric, while `gbm_grid` lets us evaluate the model with different characteristics:
```{r gbm_prep}
# Use 10-fold cross-validation with 3-repeats as our evaluation procedure
# (instead of the default "bootstrap").
gbm_control = trainControl(method = "repeatedcv",
                           number = 10L,
                           repeats = 3L,
                           # Calculate class probabilities.
                           classProbs = TRUE,
                           # Indicate that our response varaible is binary.
                           summaryFunction = twoClassSummary) 


gbm_grid = expand.grid(
  # Number of trees to fit, aka boosting iterations
  n.trees = seq(100, 1000, by = 300),
  # Depth of the decision tree (how many levels of splits).
	interaction.depth = c(1L, 3L, 5L), 
  # Learning rate: lower means the ensemble will adapt more slowly.
	shrinkage = c(0.001, 0.01, 0.1),
  # Stop splitting a tree if we only have this many obs in a tree node.
	n.minobsinnode = 10L)

# How many combinations of settings do we end up with?
nrow(gbm_grid)
```

Fit the model. Note that we will now use area under the ROC curve (called "AUC") as our performance metric, which relates the number of true positives (sensitivity) to the number of true negatives (specificity).  

> NOTE: This will take a few minutes to complete! See the .html file for the output.

```{r gbm_fit, cache = TRUE}
library(caret)
library(pROC)
set.seed(1)

# Convert our numeric indicators (1s and 0s) back into factors ("pos" and "neg")
trainlab_factor = factor(ifelse(train_label == 1, "pos", "neg"))
testlab_factor = factor(ifelse(test_label == 1, "pos", "neg"))
table(trainlab_factor, train_label)
table(testlab_factor, test_label)

# cbind: caret expects the Y response and X predictors to be part of the same dataframe
gbm1 = train(trainlab_factor ~ ., data = cbind(trainlab_factor, train_x), 
             # Use gradient boosted machine ("gbm") algorithm.
             method = "gbm",
             # Use "AUC" as our performance metric, which caret incorrectly calls "ROC"
             metric = "ROC",
             # Specify our cross-validated performance metric settings.
             trControl = gbm_control,
             # Define our gbm model tuning grid.
             tuneGrid = gbm_grid,
             # Hide detailed output (setting to TRUE will print that output).
             verbose = FALSE)

# See how long this algorithm took to complete.
gbm1$times 

# Review model summary table.
gbm1

# Plot the performance across all hyperparameter combinations.
ggplot(gbm1) + theme_bw() + ggtitle("GBM hyperparameter comparison") 
# ggsave("gbm tuning comparison.png")

# Plot variable importance.
summary(gbm1, las = 2)

# Generate predicted labels.
gbm_predicted = predict(gbm1, test_x)

# Generate class probabilities.
gbm_probs = predict(gbm1, test_x, type = "prob")

# View final model
(gbm_cm = confusionMatrix(gbm_predicted, testlab_factor))

# Define ROC characteristics
(rocCurve = roc(response = testlab_factor,
                predictor = gbm_probs[, "neg"],
                levels = rev(levels(testlab_factor)),
                auc = TRUE, ci = TRUE))

# Plot ROC curve with optimal threshold.
plot(rocCurve, print.thres = "best", main = "GBM", col = "blue") 
# ggsave("gbm ROC.png")

```

Also check out the ["xgboost" R package](https://cran.r-project.org/web/packages/xgboost/index.html) for a more powerful way to boost your trees.  

##### Challenge 5
**Big question 5:** What are some defining characteristics of the algorithms we have covered in these five exercises?

# 6. Ensemble methods
You have learned some of the characteristics for fitting several individual algorithms and have explored a little about how you can define their different (hyper)parameters. However, the ["SuperLearner" R package](https://cran.r-project.org/web/packages/SuperLearner/index.html) is a method that simplifies ensemble learning by allowing you to simultaneously evaluate the cross-validated performance of multiple algorithms and/or a single algorithm with differently tuned hyperparameters.  

Let's see how the four classification algorithms you learned in this workshop (KNN, decision tree, random forest, and gradient boosted machines) compare to each other and also to binary logistic regression (`glm`) and to the mean of Y as a benchmark algorithm, in terms of their cross-validated error!  

A "wrapper" is a short function that adapts an algorithm for the SuperLearner package. Check out the different algorithm wrappers offered by SuperLearner:
```{r}
library(SuperLearner)
listWrappers()
```

Instead of splitting the data like before, since we are using cross-validation we actually want to use the entire `pidd` dataset - cross-validation will perform as many training and test splits as necessary (this is called the number of "folds") for us! 
```{r}
str(pidd)

# Convert our Y variable to integer type.
y_int = as.integer(pidd$diabetes == "pos")

# Compile the algorithm wrappers to be used.
sl_lib = c("SL.xgboost", "SL.glm", "SL.knn", "SL.mean", "SL.rpart", "SL.ranger")
```

Fit the ensemble:

```{r cvsl_fit, cache = TRUE}
library(SuperLearner)
# This is a seed that is compatible with multicore parallel processing.
# See ?set.seed for more information.
set.seed(1, "L'Ecuyer-CMRG") 

# This will take a few minutes to execute - take a look at the .html file to see the output!
cv_sl = CV.SuperLearner(Y = y_int, X = data_scaled, verbose = TRUE,
                        SL.library = sl_lib, family = binomial(),
                        cvControl = list(V = 10L, stratifyCV = TRUE))
summary(cv_sl)
```

> NOTE: Again, this will take a few minutes to complete! See the .html file for the output!

Risk is a performance estimate - it's the average loss, and loss is how far off the prediction was for an individual observation. The lower the risk, the fewer errors the model makes in its prediction. SuperLearner's default loss metric is squared error $(y_{actual} - y_{predicted})^2$, so the risk is the mean-squared error (just like in ordinary least _squares_ regression). View the summary, plot results, and compute the AUC!
```{r cvsl_review}

# Plot the cross-validated risk estimate.
plot(cv_sl) + theme_minimal()
# ggsave("SuperLearner.pdf")

# Compute AUC for all estimators.
auc_table(cv_sl)

# Plot the ROC curve for the best estimator.
plot_roc(cv_sl)

# Review weight distribution for the SuperLearner
print(cvsl_weights(cv_sl), row.names = FALSE)
```

"Discrete SL" is when the SuperLearner chooses the single algorithm with the lowest risk. "SuperLearner" is a weighted average of multiple algorithms, or an "ensemble". In theory the weighted-average should have a little better performance, although they often tie. In this case we only have a few algorithms so the difference is minor.  

**Big question 6:** Why do you want to consider ensemble methods for your machine learning projects instead of a single algorithm?  

##### Challenge 6
1. What are the elements of the `cv_sl` object? Take a look at 1 or 2 of them. Hint: use the `names()` function to list the elements of an object, then `$` to access them (just like a dataframe).

A longer tutorial on SuperLearner is available here: (https://github.com/ck37/superlearner-guide)