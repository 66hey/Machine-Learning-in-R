---
title: "Introduction to Machine Learning in R"
author: "Evan Muzzall and Chris Kennedy"
date: "2/15/2019"
output:
  html_document:
    toc: yes
    toc_float: yes
---

# Algorithm walkthroughs


# 3. Random forests
The random forest algorithm seeks to improve on the performance of a single decision tree by taking the average of many trees. Thus, a random forest can be viewed as an **ensemble** method, or model averaging approach. The algorithm was invented by UC Berkeley's own Leo Breiman in 2001, who was also a co-creator of decision trees (see his [1984 CART book](https://www.amazon.com/Classification-Regression-Wadsworth-Statistics-Probability/dp/0412048418)).  

Random forests are an extension of **bagging**, in which multiple samples of the original data are drawn with replacement (aka "bootstrap samples"). An algorithm is fit separately to each sample, then the average of those estimates is used for prediction. While bagging can be used by any algorithm, random forest uses decision trees as its base learner. Random forests add another level of randomness by also randomly sampling the features (or covariates) at each split in each decision tree. This makes the decision trees use different covariates and therefore be more unique. As a result, the average of these trees tends to be more accurate overall.

Fit a random forest model that predicts the number of people with diabetes using the other variables as our X predictors. If our Y variable is a factor, randomForest will by default perform classification; if it is numeric/integer regression will be performed and if it is omitted it will become unsupervised! 
```{r rf_fit}
set.seed(1)
(rf1 = randomForest::randomForest(as.factor(train_label) ~ ., 
                   data = train_x, 
                   # Number of trees
                   ntree = 500, 
                   # Number of variables randomly sampled as candidates at each split.
                   mtry = 2, 
                   # We want the importance of predictors to be assessed.
                   importance = TRUE))
```

The "OOB estimate of error rate" shows us how accurate our model is. $accuracy = 1 - error rate$. OOB stands for "out of bag" - and bag is short for "bootstrap aggregation". So OOB estimates performance by comparing the predicted outcome value to the actual value across all trees using only the observations that were not part of the training data for that tree.

We can examine the relative variable importance in table and graph form. Random Forest estimates variable importance by separately examining each variable and estimating how much the model's accuracy drops when that variable's values are randomly shuffled (permuted). The shuffling temporarily removes any relationship between that covariate's value and the outcome. If a variable is important then the model's accuracy will suffer a large drop when it is randomly shuffled. But if the model's accuracy doesn't change it means the variable is not important to the model - e.g. maybe it was never even chosen as a split in any of the decision trees.
```{r rf_varImpPlot}
varImpPlot(rf1)

# Raw data
rf1$importance
```

You can read up on the [gini coefficient](https://en.wikipedia.org/wiki/Gini_coefficient) if interested. It's basically a measure of diversity or dispersion - a higher gini means the model is classifying better. The gini version does not randomly shuffle the variable values.  

Now, the goal is to see how the model performs on the test dataset:
```{r}
# This will predict the outcome class.
predicted_label = predict(rf1, newdata = test_x)
table(predicted_label, test_label)
```

Check the accuracy of the test set:
```{r prob_hist}
mean(predicted_label == test_label) 

# We can also generated probability predictions, which are more granular.
predicted_prob = as.data.frame(predict(rf1, newdata = test_x, type = "prob"))
colnames(predicted_prob) = c("no", "yes")
summary(predicted_prob)
ggplot(predicted_prob, aes(x = yes)) + geom_histogram() + theme_minimal()

# devtools::install_github("ck37/ck37r")

# Review number of terminal nodes (aka "leaves") across the decision trees.
summary(ck37r::rf_count_terminal_nodes(rf1))
```

How did it do? Are the accuracies for the training and test sets similar?  

**Big question 3:** Why is the random forest algorithm preferred to a single decision tree or bagged trees?

##### Challenge 3

1. Try a few other values of mtry - can you find one that has improved performance?
2. Maxnodes is another tuning parameter for randomForest - does changing it improve your performance?
3. Use the iris dataset to perform classification on the "Species" variable. What are you noticing about model fits between the pidd and iris datasets? 

# 4. Boosting
from [Freund Y, Schapire RE. 1999. A short introduction to boosting. Journal of Japanese Society for Artificial Intelligence 14:771-780](https://cseweb.ucsd.edu/~yfreund/papers/IntroToBoosting.pdf):  

"Boosting is a general method for improving the accuracy of any given learning algorithm" and evolved from AdaBoost and PAC learning (p. 1-2). Gradient boosted machines are ensembles decision tree methods of "weak" trees that are just slightly more accurate than random guessing. These are then "boosted" into "strong" learners. That is, the models don't have to be accurate over the entire feature space.  

The model first tries to predict each value in a dataset - the cases that can be predicted easily are _downweighted_ so that the algorithm does not try as hard to predict them.  

However, the cases that the model has difficulty predicting are _upweighted_ so that the model more assertively tries to predict them. This continues for multiple "boosting iterations", with a training-based performance measure produced at each iteration. This method can drive down generalization error (p. 5). 

Rather than testing only a single model at a time, it is useful to tune the parameters of that single model against multiple versions. Also, bootstrap is the default, but we want cross-validation.  

First create two objects - `gbm_control` and `gbm_grid`. `gbm_control` will allow us to tune the cross-validated performance metric, while `gbm_grid` lets us evaluate the model with different characteristics:
```{r gbm_prep}
# Use 10-fold cross-validation with 3-repeats as our evaluation procedure
# (instead of the default "bootstrap").
gbm_control = trainControl(method = "repeatedcv",
                           number = 10L,
                           repeats = 3L,
                           # Calculate class probabilities.
                           classProbs = TRUE,
                           # Indicate that our response varaible is binary.
                           summaryFunction = twoClassSummary) 


gbm_grid = expand.grid(
  # Number of trees to fit, aka boosting iterations
  n.trees = seq(100, 1000, by = 300),
  # Depth of the decision tree (how many levels of splits).
	interaction.depth = c(1L, 3L, 5L), 
  # Learning rate: lower means the ensemble will adapt more slowly.
	shrinkage = c(0.001, 0.01, 0.1),
  # Stop splitting a tree if we only have this many obs in a tree node.
	n.minobsinnode = 10L)

# How many combinations of settings do we end up with?
nrow(gbm_grid)
```

Fit the model. Note that we will now use area under the ROC curve (called "AUC") as our performance metric, which relates the number of true positives (sensitivity) to the number of true negatives (specificity).  

> NOTE: This will take a few minutes to complete! See the .html file for the output.

```{r gbm_fit, cache = TRUE}
set.seed(1)

# Convert our numeric indicators (1s and 0s) back into factors ("pos" and "neg")
trainlab_factor = factor(ifelse(train_label == 1, "pos", "neg"))
testlab_factor = factor(ifelse(test_label == 1, "pos", "neg"))
table(trainlab_factor, train_label)
table(testlab_factor, test_label)

# cbind: caret expects the Y response and X predictors to be part of the same dataframe
gbm1 = caret::train(trainlab_factor ~ ., data = cbind(trainlab_factor, train_x), 
             # Use gradient boosted machine ("gbm") algorithm.
             method = "gbm",
             # Use "AUC" as our performance metric, which caret incorrectly calls "ROC"
             metric = "ROC",
             # Specify our cross-validated performance metric settings.
             trControl = gbm_control,
             # Define our gbm model tuning grid.
             tuneGrid = gbm_grid,
             # Hide detailed output (setting to TRUE will print that output).
             verbose = FALSE)

# See how long this algorithm took to complete.
gbm1$times 

# Review model summary table.
gbm1

# Plot the performance across all hyperparameter combinations.
ggplot(gbm1) + theme_bw() + ggtitle("GBM hyperparameter comparison") 
# ggsave("gbm tuning comparison.png")

# Plot variable importance.
summary(gbm1, las = 2)

# Generate predicted labels.
gbm_predicted = predict(gbm1, test_x)

# Generate class probabilities.
gbm_probs = predict(gbm1, test_x, type = "prob")

# View final model
(gbm_cm = confusionMatrix(gbm_predicted, testlab_factor))

# Define ROC characteristics
(rocCurve = pROC::roc(response = testlab_factor,
                predictor = gbm_probs[, "neg"],
                levels = rev(levels(testlab_factor)),
                auc = TRUE, ci = TRUE))

# Plot ROC curve with optimal threshold.
plot(rocCurve, print.thres = "best", main = "GBM", col = "blue") 
# ggsave("gbm ROC.png")

```

Also check out the ["xgboost" R package](https://cran.r-project.org/web/packages/xgboost/index.html) for a more powerful way to boost your trees.  

##### Challenge 4
**Big question 4:** What are some defining characteristics of the algorithms we have covered in these five exercises?

# 5. Ensemble methods
You have learned some of the characteristics for fitting several individual algorithms and have explored a little about how you can define their different (hyper)parameters. However, the ["SuperLearner" R package](https://cran.r-project.org/web/packages/SuperLearner/index.html) is a method that simplifies ensemble learning by allowing you to simultaneously evaluate the cross-validated performance of multiple algorithms and/or a single algorithm with differently tuned hyperparameters.  

Let's see how the four classification algorithms you learned in this workshop (KNN, decision tree, random forest, and gradient boosted machines) compare to each other and also to binary logistic regression (`glm`) and to the mean of Y as a benchmark algorithm, in terms of their cross-validated error!  

A "wrapper" is a short function that adapts an algorithm for the SuperLearner package. Check out the different algorithm wrappers offered by SuperLearner:
```{r}
SuperLearner::listWrappers()
```

Instead of splitting the data like before, since we are using cross-validation we actually want to use the entire `pidd` dataset - cross-validation will perform as many training and test splits as necessary (this is called the number of "folds") for us! 
```{r}
str(pidd)

# Convert our Y variable to integer type.
y_int = as.integer(pidd$diabetes == "pos")

# Compile the algorithm wrappers to be used.
sl_lib = c("SL.xgboost", "SL.glm", "SL.knn", "SL.mean", "SL.rpart", "SL.ranger")
```

Fit the ensemble:

```{r cvsl_fit, cache = TRUE}
# This is a seed that is compatible with multicore parallel processing.
# See ?set.seed for more information.
set.seed(1, "L'Ecuyer-CMRG") 

# This will take a few minutes to execute - take a look at the .html file to see the output!
cv_sl = SuperLearner::CV.SuperLearner(Y = y_int, X = subset(pidd, select = -diabetes), verbose = TRUE,
                        SL.library = sl_lib, family = binomial(),
                        cvControl = list(V = 10L, stratifyCV = TRUE))
summary(cv_sl)
```

> NOTE: Again, this will take a few minutes to complete! See the .html file for the output!

Risk is a performance estimate - it's the average loss, and loss is how far off the prediction was for an individual observation. The lower the risk, the fewer errors the model makes in its prediction. SuperLearner's default loss metric is squared error $(y_{actual} - y_{predicted})^2$, so the risk is the mean-squared error (just like in ordinary least _squares_ regression). View the summary, plot results, and compute the AUC!
```{r cvsl_review}

# Plot the cross-validated risk estimate.
plot(cv_sl) + theme_minimal()
# ggsave("SuperLearner.pdf")

# Compute AUC for all estimators.
auc_table(cv_sl)

# Plot the ROC curve for the best estimator.
plot_roc(cv_sl)

# Review weight distribution for the SuperLearner
print(cvsl_weights(cv_sl), row.names = FALSE)
```

"Discrete SL" is when the SuperLearner chooses the single algorithm with the lowest risk. "SuperLearner" is a weighted average of multiple algorithms, or an "ensemble". In theory the weighted-average should have a little better performance, although they often tie. In this case we only have a few algorithms so the difference is minor.  

**Big question 5:** Why do you want to consider ensemble methods for your machine learning projects instead of a single algorithm?  

##### Challenge 5
1. What are the elements of the `cv_sl` object? Take a look at 1 or 2 of them. Hint: use the `names()` function to list the elements of an object, then `$` to access them (just like a dataframe).

A longer tutorial on SuperLearner is available here: (https://github.com/ck37/superlearner-guide)

# Bonus algorithm example - Ordinary least squares (OLS) regression
OLS can be used when the target Y variable is continuous. Remember that under the hood, `lm` is one-hot encoding factors to indicators, but try writing it out for practice if you get the chance. 



Mean squared error (MSE) and root mean squared error (RMSE) will be our performance metrics. MSE measures the difference between observed and expected values, with smaller values indicative of greater predictive accuracy. The advantage of RMSE is that it can be easier to interpret and explain because it is on the same unit scale as the outcome variable we are predicting. 

Here is an example that predicts age from the `pidd` dataset:
```{r}
# Define Y response variable
Y_reg = pidd$age

# Remove outcome variable from the X dataframe
X_reg = subset(pidd, select = -age)

# This will convert factors to indicators but will also add an extra constant column for estimating the intercept
X_reg = data.frame(model.matrix( ~ ., X_reg))

str(X_reg)

# Remove the extra intercept column, we don't need to store it in our dataset
X_reg = X_reg[, -1]

str(X_reg)

# Fit the regression model; lm() will automatically add a temporary intercept column
reg_fit = lm(Y_reg ~ ., data = X_reg)

# View the output
summary(reg_fit) 

# Predict outcome for the training data
reg_predicted = predict(reg_fit, X_reg)

# Calculate mean-squared error
MSE_reg = mean((Y_reg - reg_predicted)^2)

MSE_reg
sqrt(MSE_reg) # RMSE
```

**Bonus big question:** What might you surmise about linear regression and the pidd and iris datasets? 

##### Bonus challenge
This time, we will use the entire `iris` dataset since we can use the `predict` R function to generate pseudo-test data. Here, we can just define our Y outcome inside the function (iris$Species) and then subset the rest of iris to exclude Species as a way to define our predictors.  

Code a regression model that predicts one of the numeric variables from the "iris" dataset. 