# 10-take-home-challenge - solution
# Putting it all together :^)

### 

Using the variable names provided below, complete the code 


- How well can randomly chosen algorithms use population, life expectancy, and gdp per capita income (USD) to predict whether a country is located in the Americas versus the rest of the world? 

## 1. Clear your workspace
```{r}
rm(list = ls())
```

## 2. Load libraries and data
```{r}
library(caret)
library(ck37r)
library(ggplot2)
library(SuperLearner)
gap = read.csv("data-raw/gapminder-FiveYearData.csv")
```

## 3. No missing data
```{r}
colSums(is.na(gap))
```

## 4. Drop country and year
```{r}
gap = subset(gap, select = -c(country, year))
head(gap)
```

## 5. Collapse factor levels
```{r}
levels(gap$continent) = list(
  ## 
  "Americas" = "Americas", 
  ## 
  "World" = c("Africa", "Asia", "Europe", "Oceania"))

l_sum = merge(
  data.frame(table(gap$continent)), 
  data.frame(prop.table(table(gap$continent))), 
  by = "Var1")

levels(gap$continent)

colnames(l_sum) = c("Location", "Freq", "%")

l_sum
```

## 6. Recode y to binary
```{r}
y = ifelse(gap$continent == "Americas", 1, 0)
gap$bin = y
str(gap)

table(gap$bin, gap$continent)

##
##
##LIST
## Separate outcomes from the covariate data.
## test_outcomes = data.frame(
##   sex_fac = test$Sex,
##   is_female = as.numeric(test$Sex == "F")
## )
```

## 7. Define x
```{r}
x = subset(gap, select = -continent)
str(x)
```

## 8. Create the task
```{r}
gap_task = list(
  data = x, 
  outcome = "bin"
)
gap_task$covariates = setdiff(names(gap_task$data), gap_task$outcome)

gap_task

head(gap_task$data)
gap_task$covariates
table(gap_task$outcome)
```

## 9. Define training rows
```{r}
training_rows = caret::createDataPartition(gap_task$data[[gap_task$outcome]],
                             p = 0.70, list = FALSE) 
gap_task$train_rows = training_rows
```

Define training and testing x and y variables using `gap_task`
```{r}
train_x = gap_task$data[gap_task$train_rows, gap_task$covariates]
train_y = gap_task$data[gap_task$train_rows, gap_task$outcome]

test_x = gap_task$data[-gap_task$train_rows, gap_task$covariates]
test_y = gap_task$data[-gap_task$train_rows, gap_task$outcome]

table(train_y)
length(train_y) == length(train_x)

table(test_y)
length(test_y) == nrow(test_x)
```

### 9. Define algorithms
```{r}
gap_lib = c("SL.mean", "SL.bayesglm", "SL.knn", 
            "SL.lm", "SL.glm", "SL.glmnet", "SL.gam", "SL.earth", "SL.polymars", "SL.step", 
            "SL.rpart", "SL.ranger", "SL.randomForest", "SL.xgboost",
            "SL.nnet", 
            "SL.lda"
            )
```

# 10. Fit model
```{r}
set.seed(1, "L'Ecuyer-CMRG") 

## MAcBook Pro benchmark (2.5 GHz Intel Core i7, 16 GB 1600 MHz DDR3)
##    user      system    elapsed 
###   134.249   3.297     136.343 
## 

## Benten benchmark: 
## 
## 

system.time(
  {
    gap_cv =
      SuperLearner::CV.SuperLearner(Y = train_y, X = train_x,
                                    verbose = FALSE,
                                    SL.library = gap_lib, family = binomial(),
                                    cvControl = list(V = 5L, stratifyCV = TRUE))
  }
)

summary(gap_cv)
```

```{r}
plot(gap_cv) + theme_minimal()
```

```{r}
auc_table(gap_cv)
```

```{r}
plot_roc(gap_cv)
```

```{r}
print(cvsl_weights(gap_cv), row.names = FALSE)
```

